{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAhmadS/NLP_HW2/blob/main/NLP_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKugniAeQcGt",
        "outputId": "810a2d1c-0d17-40bd-f66f-28992dc2542f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/NLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRCx-_IwWvxT",
        "outputId": "b611ac4d-3a85-444c-9921-7952f12c1df4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extracting essential data\n",
        "\n"
      ],
      "metadata": {
        "id": "LYf9wqwEYwO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Essential packages"
      ],
      "metadata": {
        "id": "njh0oGO4Nr7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "n2iCwQ7T3uBu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#installing the necessary libraries\n",
        "!pip install -q hazm\n",
        "!pip install -q dadmatools\n",
        "!pip install -q googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "znLvol8uYzV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34fd9931-df04-49b5-b74d-def370b8d857"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.7/316.7 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 KB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.6/862.6 KB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 KB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 KB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.5/802.5 KB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.4/390.4 KB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 KB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 KB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 KB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 KB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the necessary libraries\n",
        "from __future__ import unicode_literals\n",
        "import hazm\n",
        "\n",
        "import dadmatools.pipeline.language as language\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "vJ1FDzA_Y5VW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating dataset"
      ],
      "metadata": {
        "id": "QU-8QbkZN4Pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Nlp HW2/corpus.txt\",\"rb\") as f:\n",
        "  corpo = pickle.load(f)\n",
        "\n",
        "corpo"
      ],
      "metadata": {
        "id": "wtvAazXLhkjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the corpus used for this task comes from hazm word list\n",
        "corpus = list(set(corpo))\n",
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLtriJNNo35v",
        "outputId": "af772381-9751-45c5-de4b-77b1e31870dc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35316"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install sentencepiece\n",
        "# !pip install transformers\n",
        "# from transformers import AutoTokenizer\n",
        "# from transformers import AutoModelForTokenClassification  # for pytorch\n",
        "# from transformers import TFAutoModelForTokenClassification  # for tensorflow\n",
        "# from transformers import pipeline\n",
        "# # model_name_or_path = \"HooshvareLab/bert-fa-zwnj-base-ner\"  # Roberta\n",
        "# # model_name_or_path = \"HooshvareLab/roberta-fa-zwnj-base-ner\"  # Roberta\n",
        "# model_name_or_path = \"HooshvareLab/distilbert-fa-zwnj-base-ner\"  # Distilbert\n",
        "# # model_name_or_path = \"HooshvareLab/albert-fa-zwnj-base-v2-ner\"  # Albert\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "\n",
        "# # tokenizer=hazm.WordTokenizer\n",
        "# model = AutoModelForTokenClassification.from_pretrained(model_name_or_path)  # Pytorch\n",
        "# # model = TFAutoModelForTokenClassification.from_pretrained(model_name_or_path)  # Tensorflow\n",
        "\n",
        "# ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "QxGwIUZP6aMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AN8OwAC7ANC",
        "outputId": "0d2b6364-5d0f-40e3-a931-881d6a4dfa53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35316"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# persian_names=pd.read_csv(\"Nlp HW2/Persian names.csv\")\n",
        "# persian_names_list=persian_names[\"first_name\"]\n",
        "# persian_names_list=persian_names_list.tolist()\n",
        "# for word in corpus:\n",
        "#   ner_res=ner(word)\n",
        "#   if len(ner_res)==0:\n",
        "#     continue\n",
        "#   if ner(word)[0][\"entity\"]==\"B-PER\" and word not in persian_names_list:\n",
        "#     print(word)\n",
        "#     corpus.remove(word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEeWAK2k5ndm",
        "outputId": "cbe1bf79-64a6-4522-f65a-4b521902817f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "سعید\n",
            "پینوشه\n",
            "زبیده\n",
            "سلیفون\n",
            "گاییده\n",
            "ریچی\n",
            "خان‌زاده\n",
            "جاسیگاری\n",
            "عطاکننده\n",
            "ژیگول\n",
            "دب\n",
            "خودباور\n",
            "لارسن\n",
            "سروشکل\n",
            "رامبد\n",
            "شوبرت\n",
            "جمشید\n",
            "ژولیوس\n",
            "ذبیح‌الله\n",
            "ابن‌سینا\n",
            "رضوی\n",
            "خشایار\n",
            "بردیا\n",
            "روبرت\n",
            "سه‌تایکی\n",
            "سنفونی\n",
            "دون‌همتی\n",
            "مجید\n",
            "لویی\n",
            "رونالد\n",
            "پارسامنش\n",
            "پالایه\n",
            "یورگن\n",
            "برنیامده\n",
            "الیوت\n",
            "بل\n",
            "دکارت\n",
            "فراست\n",
            "حسین\n",
            "کریس\n",
            "ابدالدهر\n",
            "قلی‌خسروی\n",
            "مصطفی‌زاده\n",
            "ابراهیم\n",
            "رابین\n",
            "مرویات\n",
            "گارسیا\n",
            "کروبی\n",
            "ماترک\n",
            "طاهرزاده\n",
            "حسنعلی\n",
            "شجری\n",
            "بن‌لادن\n",
            "میترال\n",
            "رضیع\n",
            "پرویز\n",
            "آزاده‌پرور\n",
            "رضاع\n",
            "خضرا\n",
            "فرحزاد\n",
            "رزیتا\n",
            "خیام‌شناس\n",
            "یدالله\n",
            "شوخ\n",
            "مقدم\n",
            "نیل\n",
            "مکبث\n",
            "سید\n",
            "محب\n",
            "پیمبرزادگی\n",
            "هاردی\n",
            "تاج‌الدین\n",
            "شمس‌الدین\n",
            "ریغو\n",
            "خشایارشا\n",
            "زانتیا\n",
            "ایزدپناه\n",
            "معین‌الدین\n",
            "ژاک\n",
            "کریستف\n",
            "زرگر\n",
            "نعلبکی\n",
            "تین\n",
            "حلال‌زاده\n",
            "پنس\n",
            "کنفی\n",
            "رضازاده\n",
            "آنجل\n",
            "شریف\n",
            "هم‌پالکی\n",
            "زید\n",
            "هماورد\n",
            "ژرژت\n",
            "پریسا\n",
            "ایگور\n",
            "معاویه\n",
            "قلیل\n",
            "آهکی\n",
            "شیزوفرنی\n",
            "عبدالملک\n",
            "خواجگان\n",
            "جهانشاهی\n",
            "خونخواه\n",
            "طغرل\n",
            "مرحبا\n",
            "دولتشاهی\n",
            "زکریا\n",
            "محمدی\n",
            "بکهام\n",
            "هان\n",
            "ریدن\n",
            "هیلتون\n",
            "ظهیرالدوله\n",
            "چمران\n",
            "فریبا\n",
            "بنیامین\n",
            "تام\n",
            "روفرشی\n",
            "رقعی\n",
            "اسعد\n",
            "محبس\n",
            "خسروپرویز\n",
            "شایگان\n",
            "میلتون\n",
            "الینا\n",
            "میرزاده\n",
            "مارقین\n",
            "آبدارچی\n",
            "آقاخانی\n",
            "سنجر\n",
            "طباطبایی\n",
            "جوزپه\n",
            "انوری\n",
            "آسودن\n",
            "سروانتس\n",
            "الله‌آباد\n",
            "اسکندرنامه\n",
            "احدی\n",
            "کاکاوند\n",
            "منیریه\n",
            "علاف\n",
            "داوکینز\n",
            "صادقیه\n",
            "اریک\n",
            "لطفعلی\n",
            "روگذر\n",
            "روح‌الله\n",
            "سیف\n",
            "کلمنت\n",
            "وحیدیه\n",
            "صفرعلی\n",
            "هارون‌الرشید\n",
            "گاندی\n",
            "هوچی\n",
            "زرشناس\n",
            "شمس‌العماره\n",
            "آریانپور\n",
            "فرانچسکو\n",
            "فیثاغورث\n",
            "نیکلای\n",
            "رخشیده\n",
            "میکاییل\n",
            "فردین\n",
            "لوران\n",
            "پوتین\n",
            "رودولف\n",
            "نصرالدین\n",
            "حکم‌آباد\n",
            "خیام\n",
            "جرجیا\n",
            "داوودی\n",
            "کاسترو\n",
            "ابراهیم‌خان\n",
            "دادخواه\n",
            "کلودیو\n",
            "هشت‌صدمیلیونی\n",
            "سلیمان‌وار\n",
            "نورالانوار\n",
            "عطار\n",
            "هال\n",
            "بیگاری\n",
            "قاسم‌خانی\n",
            "غلامحسین\n",
            "برمکی\n",
            "مفتح\n",
            "رادمنش\n",
            "محمدیار\n",
            "ایهاالناس\n",
            "سپانلو\n",
            "وفاجو\n",
            "مشیر\n",
            "فلمینگ\n",
            "میرداماد\n",
            "تهنیت‌گو\n",
            "باقرقره\n",
            "آلبالوپلو\n",
            "داریوش\n",
            "پشم‌ریس\n",
            "مورفین\n",
            "فهد\n",
            "اسمیت\n",
            "میناکار\n",
            "امین\n",
            "زادهرمز\n",
            "کبریایی\n",
            "توتیا\n",
            "محدث\n",
            "ماسیده\n",
            "اوحدی\n",
            "راعی\n",
            "خدیجه\n",
            "لطف‌آمیز\n",
            "اشدا\n",
            "لوییس\n",
            "سالفان\n",
            "متظاهر\n",
            "نوشین\n",
            "عمادالدین\n",
            "ژاژخا\n",
            "رشید\n",
            "سین‌جیم\n",
            "مجیدیه\n",
            "انتظام\n",
            "بوش\n",
            "چخوف\n",
            "راجر\n",
            "برهان‌الدین\n",
            "کمبوجیه\n",
            "چسان\n",
            "فرح‌آباد\n",
            "سعیده\n",
            "زکام\n",
            "آدام\n",
            "کاتوره\n",
            "محمدابراهیم\n",
            "ویلسون\n",
            "شیرمحمدی\n",
            "رسول‌الله\n",
            "هانتینگتون\n",
            "شهبال\n",
            "شجر\n",
            "گاییدن\n",
            "امیرسلیمانی\n",
            "کول\n",
            "بلوند\n",
            "گوته\n",
            "چنی\n",
            "محی‌الدین\n",
            "فیلیپه\n",
            "دونالد\n",
            "چوبک\n",
            "فرخزاد\n",
            "حاجیه\n",
            "جارچی\n",
            "کاتوزیان\n",
            "زورچپانی\n",
            "خورخه\n",
            "فرزین\n",
            "کاشکی\n",
            "زری‌دوزی\n",
            "پولادین\n",
            "بیلچه\n",
            "کواکب\n",
            "صالحیه\n",
            "آندرانیک\n",
            "پیتر\n",
            "اباطیل\n",
            "نقشینه\n",
            "یعقوب\n",
            "یاسر\n",
            "عزیزی\n",
            "مجتبوی\n",
            "لیزی\n",
            "الیزابت\n",
            "بروس\n",
            "کامران\n",
            "زرباف\n",
            "گوردون\n",
            "نصرت‌الله\n",
            "سباستین\n",
            "هداوند\n",
            "اویسی\n",
            "دیویی\n",
            "آلبرتا\n",
            "فریبرز\n",
            "لاهیجی\n",
            "رنجبر\n",
            "ارژنگ\n",
            "مرتضوی\n",
            "مونالیزا\n",
            "میانوند\n",
            "ایزاک\n",
            "الیور\n",
            "رضاشاه\n",
            "فیلیپ\n",
            "فاطر\n",
            "اسماعیل‌پور\n",
            "دوازده‌امامی\n",
            "کریم\n",
            "جفنگ\n",
            "گنه\n",
            "ملارد\n",
            "فخیمه\n",
            "قرقی\n",
            "مرتضی\n",
            "نجم‌آبادی\n",
            "کروز\n",
            "حاج‌بادام\n",
            "ابا\n",
            "اسماعیل‌خان\n",
            "خسروشاهی\n",
            "رعیت‌زاده\n",
            "عزالدین\n",
            "موسیر\n",
            "ربانی\n",
            "آذری‌تبار\n",
            "امیریه\n",
            "طیب\n",
            "اردشیر\n",
            "دانته\n",
            "بیهقی\n",
            "کلوین\n",
            "گراهام\n",
            "آزرم\n",
            "آذرین\n",
            "بوریابافی\n",
            "میرا\n",
            "کاظم‌آباد\n",
            "آقازاده\n",
            "ساروق\n",
            "وثوق\n",
            "آباجی\n",
            "علوی\n",
            "کیر\n",
            "سیف‌آباد\n",
            "سوفیا\n",
            "بیلقانی\n",
            "حجت‌آباد\n",
            "اشراف‌زاده\n",
            "مفلس\n",
            "سیلیکون\n",
            "کامواباف\n",
            "زلیخا\n",
            "سار\n",
            "آنتونوف\n",
            "خوارزمشاه\n",
            "پامنبری\n",
            "رجب‌بیگی\n",
            "اسکندر\n",
            "باربارا\n",
            "هار\n",
            "هفتالیان\n",
            "کامبیز\n",
            "جهانشاه\n",
            "می‌گساری\n",
            "بزرگمهر\n",
            "خاصره\n",
            "علی‌القاعده\n",
            "امیرارسلان\n",
            "ناکس\n",
            "وشمگیر\n",
            "بروجردی\n",
            "یحیا\n",
            "سیلویا\n",
            "بزرگ‌زاده\n",
            "طوس\n",
            "گیپورباف\n",
            "هون\n",
            "ساراگوسا\n",
            "مهدی‌آباد\n",
            "هولمز\n",
            "برانده\n",
            "حبیب\n",
            "دیگو\n",
            "بیل\n",
            "نیاسر\n",
            "متین\n",
            "افواه\n",
            "اردوغان\n",
            "نیکولا\n",
            "محمدامین\n",
            "امامعلی\n",
            "زنجان‌پور\n",
            "زمانمند\n",
            "راشدی\n",
            "آل‌پاچینو\n",
            "سپور\n",
            "توماس\n",
            "امشی\n",
            "محمدیان\n",
            "حمید\n",
            "سالومه\n",
            "رحیم\n",
            "دیمیتری\n",
            "گلباف\n",
            "ماندلا\n",
            "ادیسون\n",
            "ژیلت\n",
            "گنجعلی\n",
            "اندرسون\n",
            "بارنابا\n",
            "فردریک\n",
            "عبدالرحیم\n",
            "چیذر\n",
            "زیلو\n",
            "سکرت\n",
            "دنیلسون\n",
            "گیپوربافی\n",
            "بیرانوند\n",
            "لوتر\n",
            "نوشکفته\n",
            "وعده‌ووعید\n",
            "چهاراکتاوی\n",
            "بدلهجه\n",
            "شمسی‌پور\n",
            "قطب‌الدین\n",
            "زبیر\n",
            "منگل\n",
            "جادگمه\n",
            "عمادالدوله\n",
            "بهنود\n",
            "ژوزف\n",
            "ژان\n",
            "افراشتن\n",
            "گیتس\n",
            "ابن‌بطوطه\n",
            "محسن‌آباد\n",
            "سلطان‌علی\n",
            "هیلمن\n",
            "چارچنگولی\n",
            "فلاح‌پور\n",
            "موسوی\n",
            "ماریا\n",
            "هیز\n",
            "بدسگال\n",
            "شرف‌الدین\n",
            "الکساندر\n",
            "ندوشن\n",
            "جلالیه\n",
            "نیلوفران\n",
            "مازیار\n",
            "بهروزی\n",
            "مگو\n",
            "نسوان\n",
            "تی‌تیش‌مامانی\n",
            "اسماعیل‌زاده\n",
            "پروین\n",
            "تهمورث\n",
            "شیوا\n",
            "عصار\n",
            "دیلاق\n",
            "والشمس\n",
            "رومن\n",
            "ماکسیم\n",
            "هاشم‌پور\n",
            "میازاکی\n",
            "آدینه\n",
            "جیم\n",
            "کیارستمی\n",
            "ناتاشا\n",
            "براون\n",
            "رجب‌علی\n",
            "عور\n",
            "بابا\n",
            "لاجورد\n",
            "خاویر\n",
            "فر\n",
            "کوروش\n",
            "دزدکی\n",
            "چس‌خور\n",
            "دنبلان\n",
            "فطیر\n",
            "چرچیل\n",
            "قالیباف\n",
            "ابوعطا\n",
            "دگردیس\n",
            "گورکن\n",
            "ایوب\n",
            "امیرحسین\n",
            "کوبریک\n",
            "عطاشده\n",
            "لارنس\n",
            "ضراب‌خانه\n",
            "جوزف\n",
            "مارلون\n",
            "کیا\n",
            "رخشان\n",
            "مولوی\n",
            "کامرون\n",
            "بول\n",
            "لودویگ\n",
            "محمدقلی\n",
            "مهرجویی\n",
            "ابی‌طالب\n",
            "پالان\n",
            "خاطرخواه\n",
            "فیشر\n",
            "مصطفی\n",
            "ویکتوریا\n",
            "احمدی‌نژاد\n",
            "داگلاس\n",
            "کرمی\n",
            "برایان\n",
            "مفلوج\n",
            "کمال‌الدین\n",
            "خسروشهر\n",
            "خرامیده\n",
            "میراب\n",
            "دومینیک\n",
            "لارنژیت\n",
            "ابوجعفر\n",
            "سعدی\n",
            "مک‌کین\n",
            "فولادوند\n",
            "رایت\n",
            "اقالیم\n",
            "روسو\n",
            "جرالد\n",
            "بیلقان\n",
            "فرخ‌زاد\n",
            "مینورسکی\n",
            "عطابخش\n",
            "جفاپیشگی\n",
            "هارونیه\n",
            "آناهیتا\n",
            "جانسون\n",
            "حسین‌آباد\n",
            "مولانا\n",
            "بیگدلی\n",
            "دنیس\n",
            "آنتونیو\n",
            "چارلز\n",
            "سهرورد\n",
            "دنیرو\n",
            "سنایی\n",
            "سورچی\n",
            "صفار\n",
            "یوسف\n",
            "نظریابی\n",
            "روشن‌بیان\n",
            "کسینوس\n",
            "برکلی\n",
            "اسفندیار\n",
            "قطب‌زاده\n",
            "میرعلی\n",
            "اینشتین\n",
            "میشل\n",
            "عامر\n",
            "محجورین\n",
            "منتسکیو\n",
            "شیشکی\n",
            "جمیل\n",
            "ژوکوفسکی\n",
            "زندیق\n",
            "آجی\n",
            "پوریا\n",
            "مایملک\n",
            "هریسون\n",
            "زوزن\n",
            "پاشا\n",
            "کریستین\n",
            "شفیق\n",
            "دیویس\n",
            "یوسفعلی\n",
            "اثیری\n",
            "موقف\n",
            "ابوسعید\n",
            "مکدر\n",
            "اکبیری\n",
            "گرییده\n",
            "ادخال\n",
            "دالتون\n",
            "عسگراولادی\n",
            "پویان\n",
            "کمالوند\n",
            "سهیلا\n",
            "سایمون\n",
            "پیرس\n",
            "کالین\n",
            "لوا\n",
            "بایسنقر\n",
            "انصاریان\n",
            "خاتمیت\n",
            "یزدانیان\n",
            "مندوب\n",
            "رحمانیه\n",
            "دادگرانه\n",
            "تودماغی\n",
            "ریچارد\n",
            "کاظمیه\n",
            "سفیدبخت\n",
            "ناصرخسرو\n",
            "جهانگیر\n",
            "جاوید\n",
            "هگل\n",
            "هیتلر\n",
            "کریه\n",
            "سورچرانی\n",
            "جیوش\n",
            "هفت‌صدمیلیونی\n",
            "یانکی\n",
            "قلیچ\n",
            "غزنوی\n",
            "حاجی‌آباد\n",
            "مشیرالدوله\n",
            "هوالباقی\n",
            "فروهر\n",
            "ناتالی\n",
            "زری\n",
            "حسن‌پور\n",
            "هستان‌شناس\n",
            "جولی\n",
            "فرفورژه\n",
            "استالین\n",
            "ریسوگراف\n",
            "سیمور\n",
            "بهرام‌آباد\n",
            "جلیل\n",
            "نه‌صدمیلیاردی\n",
            "احمدی\n",
            "شفیع‌زاده\n",
            "جفرسون\n",
            "همخانگی\n",
            "کینگ\n",
            "زیرالگو\n",
            "خمینی\n",
            "آیدین\n",
            "سلیمه\n",
            "کندی\n",
            "سعدیا\n",
            "آبا\n",
            "علیخانی\n",
            "گرگوری\n",
            "قاضی‌زاده\n",
            "میرقلی\n",
            "دانیال\n",
            "زهراوی\n",
            "اوهام\n",
            "عثمان\n",
            "اکبر\n",
            "سلیم\n",
            "آدامس‌فروش\n",
            "فضلا\n",
            "لنگ‌لنگان\n",
            "مؤذن‌زاده\n",
            "دنیزلی\n",
            "ادیب\n",
            "عالمیان\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3PJyZbp7jRc",
        "outputId": "ea8cc39e-48cb-49f2-ee6d-509407c89fe8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33606"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file = open('Nlp HW2/corpus_without_english_names.txt','w',encoding = 'utf-8')\n",
        "# for word in corpus:\n",
        "#   file.write(word)\n",
        "#   file.write(\"\\n\")\n",
        "# file.close()"
      ],
      "metadata": {
        "id": "w6geoVOd_NkK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#roles contains all the possible roles in our dictionary\n",
        "roles = [x[2] for x in hazm.words_list()]\n",
        "roles = list(set(itertools.chain(*roles)))\n",
        "roles"
      ],
      "metadata": {
        "id": "febyfjwQr0hc",
        "outputId": "91aaf2ab-949d-4242-ceb9-5d62c1541c0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['AJC',\n",
              " 'POSTP',\n",
              " 'AJ',\n",
              " 'PRO',\n",
              " 'PL',\n",
              " 'PR',\n",
              " 'NUM',\n",
              " 'NIN',\n",
              " 'INT',\n",
              " 'DET',\n",
              " 'PS',\n",
              " 'N',\n",
              " 'COMP',\n",
              " 'ZVR',\n",
              " 'ADV',\n",
              " 'P',\n",
              " 'CL',\n",
              " 'RES',\n",
              " 'CONJ',\n",
              " 'V']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = hazm.Lemmatizer()\n",
        "stemmer = hazm.Stemmer()"
      ],
      "metadata": {
        "id": "LZ0cqll7Zwdu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''this way of defining stop_words may not be usual,\n",
        "but the main purpose has been to eliminate disturbing words;\n",
        "such as: punctiation marks, pronouns and Persian\\Arabic grammatical signs'''\n",
        "\n",
        "stop_words = [\"ند\",\"یم\",\"ید\",\"ی\",\"تان\",\"ش\",\"ت\",\"شان\",\"مان\",\"م\",\"ها\",\"ات\",\"ان\",\"ین\",\"ون\",\"‌های\"]\n",
        "stop_words.extend([\".\",\"؛\",\",\",\":\",\"?\",\"!\",\"؟\",\"ْ\",\"ٌ\",\"ٍ\",\"ً\",\"ُ\",\"ِ\",\"َ\",\"ّ\",\"}\",\"ؤ\",\"{\",\"»\",\"«\",\"ٰ\",\"‌‌‌‌‌‌‌‌ٔ\",\"ء\",\"\\\"\",\"(\",\")\"])\n",
        "more_stopwords=[\"ها\",\"تر\",\"ترین\"]\n",
        "stop_words.extend(more_stopwords)"
      ],
      "metadata": {
        "id": "sUJqnvFEwe3v"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_list = [\"ی\",\"تان\",\"ش\",\"ت\",\"شان\",\"مان\",\"م\",\"‌ها\"]\n",
        "corpu = []\n",
        "for word in hazm.words_list():\n",
        "  corpu.append(word[0])\n",
        "  corpu.append(lemmatizer.lemmatize(word[0]))\n",
        "  if (\"N\" in word[2]) or (\"AJC\" in word[2]) or (\"AJ\" in word[2]):\n",
        "    for ele in add_list:\n",
        "      corpu.append(word[0]+ele)\n",
        "\n",
        "corpu = list(set(corpu))\n",
        "len(corpu)"
      ],
      "metadata": {
        "id": "SlN7tsHwpAyJ",
        "outputId": "e9383572-dab1-494d-ad88-3abc525dbfc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "308048"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main code"
      ],
      "metadata": {
        "id": "ezt1J32nymfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Config"
      ],
      "metadata": {
        "id": "uoztZzu6NAYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nlp is the dadmatools' pipeline used for lemmatization purpose only\n",
        "pips = 'lem' \n",
        "nlp = language.Pipeline(pips)\n",
        "\n",
        "print(nlp.analyze_pipes(pretty=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ShvxXYdXJ0Y",
        "outputId": "c80b7df1-d1c8-490e-979a-e35f1317562b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading file /root/.pernlp/fa_tokenizer.pt: : 639kB [00:04, 159kB/s]                         \n",
            "Downloading file /root/.pernlp/fa_mwt.pt: : 721kB [00:04, 169kB/s]                         \n",
            "Downloading file /root/.pernlp/fa_lemmatizer.pt: : 4.69MB [00:03, 1.45MB/s]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================= Pipeline Overview =============================\u001b[0m\n",
            "\n",
            "#   Component   Assigns       Requires   Scores   Retokenizes\n",
            "-   ---------   -----------   --------   ------   -----------\n",
            "0   tokenizer                                     True       \n",
            "                                                             \n",
            "1   lemmatize   token.lemma                       False      \n",
            "\n",
            "\u001b[38;5;2m✔ No problems found.\u001b[0m\n",
            "{'summary': {'tokenizer': {'assigns': [], 'requires': [], 'scores': [], 'retokenizes': True}, 'lemmatize': {'assigns': ['token.lemma'], 'requires': [], 'scores': [], 'retokenizes': False}}, 'problems': {'tokenizer': [], 'lemmatize': []}, 'attrs': {'token.lemma': {'assigns': ['lemmatize'], 'requires': []}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we use an args dictionary to initialize our model\n",
        "args = {\n",
        "    'corpus':corpu,\n",
        "    'pipe':nlp,\n",
        "    'stop_words':stop_words,\n",
        "    'normalizer':hazm.Normalizer(),\n",
        "    'lemmatizer':hazm.Lemmatizer(),\n",
        "    'POS_tagger':hazm.POSTagger(model='Nlp HW2/resources/postagger.model'),\n",
        "    'translator':None,\n",
        "    'word_tokenizer':hazm.WordTokenizer(),\n",
        "    'sent_tokenizer':hazm.SentenceTokenizer()\n",
        "}"
      ],
      "metadata": {
        "id": "qP0mg4wDOMIm"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model"
      ],
      "metadata": {
        "id": "6kvuGlYaNPYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#as the name suggests, the class is used to spot and show the positin of foreign words within the text\n",
        "\n",
        "class foreign_word_detector():\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      args\n",
        "      ):\n",
        "    \n",
        "    self.corpus = args['corpus']\n",
        "    self.stop_words = args['stop_words']\n",
        "    self.lang_pipe = args['pipe']\n",
        "    self.normalizer = args['normalizer']\n",
        "    self.lemmatizer = args['lemmatizer']\n",
        "    self.translator = args['translator']\n",
        "    self.word_tokenizer = args['word_tokenizer']\n",
        "    self.sent_tokenizer = args['sent_tokenizer']\n",
        "    self.tagger = args['POS_tagger']\n",
        "\n",
        "#the following function is the main funtion of the model used for detecting the foreign words and their position\n",
        "  def detect(self, text):\n",
        "    sentenced = self.sent_tokenizer.tokenize(text)\n",
        "  \n",
        "    normalized = [self.normalizer.normalize(sent) for sent in sentenced]\n",
        "    normalized = [sent.replace(\"\\u200cهای\",\"\") for sent in normalized]\n",
        "    normalized = [sent.replace(\"\\u200c\",\" \") for sent in normalized]\n",
        "    # print(f\"normalized:{normalized}\")\n",
        "    lemmed = [str(self.lang_pipe(sent)) for sent in normalized]\n",
        "    # print(f\"lemmed:{lemmed}\")\n",
        "    words_list = [self.word_tokenizer.tokenize(sent) for sent in lemmed]\n",
        "\n",
        "    final_words_list = []\n",
        "    for sent in words_list:\n",
        "      words_second_list = []\n",
        "      tags = self.tagger.tag(sent)\n",
        "      for i in range(len(tags)):\n",
        "        if tags[i][1] != \"V\":\n",
        "          words_second_list.append(tags[i][0])\n",
        "      final_words_list.append(words_second_list)\n",
        "\n",
        "    words_list = list(itertools.chain(*final_words_list))\n",
        "    words_list = [word for word in words_list if word not in self.stop_words]\n",
        "    #lemmed_words_list = [lemmatizer.lemmatize(word) for word in words_list]\n",
        "    # print(lemmed_words_list)\n",
        "\n",
        "    output = dict()\n",
        "    for word in words_list:\n",
        "      if word not in self.corpus:\n",
        "        output[word] = []\n",
        "        add_ind = 0\n",
        "\n",
        "        string = text\n",
        "        while word in string:\n",
        "\n",
        "          begin = string.find(word)\n",
        "          output[word].append((begin+add_ind,add_ind+begin+len(word)))\n",
        "\n",
        "          string = string[begin+len(word):]\n",
        "          add_ind+= begin+len(word)\n",
        "    \n",
        "    return output\n",
        "          "
      ],
      "metadata": {
        "id": "abisr-pQyq1p"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test"
      ],
      "metadata": {
        "id": "Oyh-F0TrNX2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####results"
      ],
      "metadata": {
        "id": "REhKcO_FRmuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.load_csv(\"Nlp HW2/test.xlsx\")\n",
        "texts = pickle.load(open(\"Nlp HW2/data_text.TXT\"))\n",
        "\n",
        "#join the 2 parts into 1 dataframe\n",
        "\n",
        "#create an eng_words coloumn"
      ],
      "metadata": {
        "id": "l5ixR8F9R7fs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "5362f694-2365-45d1-e5fd-6e3f6552dea7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-cc7b634d416d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nlp HW2/test.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nlp HW2/data_text.TXT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#join the 2 parts into 1 dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'load_csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####user run"
      ],
      "metadata": {
        "id": "tljaM-3WRscu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the run function, is an interactive way to map user_input to the detectors answer\n",
        "def run():\n",
        "  detector = foreign_word_detector(args)\n",
        "\n",
        "  print(\"enter your sample text.\\nenter an empty string if you want to end the process\")\n",
        "  while True : \n",
        "    text = input()\n",
        "    if text == \"\": \n",
        "      break\n",
        "    \n",
        "    output = detector.detect(text)\n",
        "    print(output)\n"
      ],
      "metadata": {
        "id": "gFnE-4jRs3Hp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvJUGGdQw1mH",
        "outputId": "6e1a1ec3-1970-4595-f94a-ef70b4475a3f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter your sample text.\n",
            "enter an empty string if you want to end the process\n",
            "هاردی\n",
            "{}\n",
            "امروز تسک هاردی داشتیم\n",
            "{'تسک': [(6, 9)]}\n",
            "من و تو و پدرت و مادرش در گاردنشان غذا خوردیم.\n",
            "{'گاردن': [(26, 31)]}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##New idea"
      ],
      "metadata": {
        "id": "hr0gxCMLNfjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using phonetics"
      ],
      "metadata": {
        "id": "cNb5-GbmEOyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We import an English dictionary. Add the english words in \"words\" and the corresponding phonetics to \"phonics_list\".\n",
        "words = []\n",
        "phonics_list = []\n",
        "dict_file = open('Nlp HW2/cmudict.dict', 'r')\n",
        "with dict_file as f:\n",
        "    phonics = [line.rstrip('\\n') for line in f]\n",
        "    for p in phonics:\n",
        "        x = p.split(' ')\n",
        "        words.append(x[0])\n",
        "        phonics_list.append(' '.join(x[1:]))"
      ],
      "metadata": {
        "id": "_kzbejEP6GP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phons_unique = list(set(itertools.chain(*[ph.split(\" \") for ph in phonics_list])))\n",
        "phons_as_list = [ph.split(\" \") for ph in phonics_list]"
      ],
      "metadata": {
        "id": "zulC4Adu6upO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Nlp HW2/phons_exchange.txt\",\"rb\") as file:\n",
        "  phons_dict = pickle.load(file)"
      ],
      "metadata": {
        "id": "vtwOxqizjC-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phons_dict[\"NG\"] = (\"ن\")\n",
        "phons_dict[\"TH\"] = (\"ت\")\n",
        "phons_dict[\"AO2\"] = phons_dict[\"AO1\"]\n",
        "phons_dict[\"IH2\"] = (\"ای\",\"ی\")\n",
        "phons_dict[\"IH0\"] = phons_dict[\"IH2\"]\n",
        "phons_dict[\"IH1\"] = phons_dict[\"IH0\"]"
      ],
      "metadata": {
        "id": "6_UTCNuVjWlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting our words to their persian correspondent with the use of phonetics in the \"persianized\"\n",
        "persianized_list = []\n",
        "persianized = []\n",
        "for x in phons_as_list:\n",
        "  word_list = []\n",
        "  count=0\n",
        "  for ph in x:\n",
        "    count+=1\n",
        "    if len(phons_dict[ph])>1:\n",
        "      if count==1:\n",
        "        word_list.append(phons_dict[ph][0])\n",
        "      else:\n",
        "        word_list.append(phons_dict[ph][1])\n",
        "    else:\n",
        "      word_list.append(phons_dict[ph][0])\n",
        "\n",
        "  persianized_list.append(word_list)\n",
        "  persianized.append(''.join(word_list))"
      ],
      "metadata": {
        "id": "TZQlhU9GjSJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking some words from \"words\"\n",
        "words[41000:41010]"
      ],
      "metadata": {
        "id": "5_pSldPjknPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the corresponding persionized words from \"persionized\"\n",
        "persianized[41000:41010]"
      ],
      "metadata": {
        "id": "4mKUNAPmkdt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if our persionized method works fine on some words.\n",
        "listt = ['contribution','thanks','computer','system','task','time','garden','burger','chop','fandrich','infinity']\n",
        "\n",
        "for i in range(len(words)):\n",
        "  if words[i] in listt:\n",
        "    print(words[i])\n",
        "    print(phons_as_list[i])\n",
        "    print(i)\n",
        "    print(persianized[i])\n",
        "    print(\"-----------------\")"
      ],
      "metadata": {
        "id": "Pm2B0nXskxX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#phons_dict contains the Persian equivalent of English phonetics\n",
        "phons_dict"
      ],
      "metadata": {
        "id": "4Gz8wC5fCZrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U strsimpy\n",
        "import pandas as pd\n",
        "from strsimpy.weighted_levenshtein import WeightedLevenshtein\n",
        "\n",
        "\n",
        "def insertion_cost(char):\n",
        "  if char==\"ا\":\n",
        "    return 0\n",
        "  return 1.0\n",
        "\n",
        "\n",
        "def deletion_cost(char):\n",
        "  if char==\"ا\":\n",
        "    return 0\n",
        "  return 1.0\n",
        "\n",
        "\n",
        "def substitution_cost(char_a, char_b):\n",
        "  if char_a==\"ا\" and char_b==\"آ\":\n",
        "    return 0\n",
        "  return 2.0\n",
        "\n",
        "weighted_levenshtein = WeightedLevenshtein(\n",
        "    substitution_cost_fn=substitution_cost,\n",
        "    insertion_cost_fn=insertion_cost,\n",
        "    deletion_cost_fn=deletion_cost)"
      ],
      "metadata": {
        "id": "wqRR8JTWpC3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w=\"هاردی\"\n",
        "for i,word in enumerate(persianized):\n",
        "  if word==w:\n",
        "    print(word)\n",
        "    print(words[i])"
      ],
      "metadata": {
        "id": "LVtELnALuGxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w=\"هاردی\"\n",
        "for i in hazm.words_list():\n",
        "  if i[0]==w:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "uuR5VnDL37ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1=\"کامپیوتر\"\n",
        "w2=\"کمپیوتر\"\n",
        "for i in persianized:\n",
        "  if i==w2:\n",
        "    print(i)\n",
        "weighted_levenshtein.distance(w1,w2)"
      ],
      "metadata": {
        "id": "Di9oMmfptFqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#as the name suggests, the class is used to spot and show the positin of foreign words within the text\n",
        "\n",
        "class phonetics_foreign_word_detector():\n",
        "\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      args\n",
        "      ):\n",
        "    \n",
        "    self.corpus = args['corpus']\n",
        "    self.stop_words = args['stop_words']\n",
        "    self.lang_pipe = args['pipe']\n",
        "    self.normalizer = args['normalizer']\n",
        "    self.lemmatizer = args['lemmatizer']\n",
        "    self.translator = args['translator']\n",
        "    self.word_tokenizer = args['word_tokenizer']\n",
        "    self.sent_tokenizer = args['sent_tokenizer']\n",
        "    self.tagger = args['POS_tagger']\n",
        "    \n",
        "  def is_foreign_word(self,word):\n",
        "    for english_word in persianized:\n",
        "      if weighted_levenshtein.distance(word,english_word)==0:\n",
        "        return True\n",
        "    return False\n",
        "      \n",
        "\n",
        "#the following function is the main funtion of the model used for detecting the foreign words and their position\n",
        "  def detect(self, text):\n",
        "    sentenced = self.sent_tokenizer.tokenize(text)\n",
        "  \n",
        "    normalized = [self.normalizer.normalize(sent) for sent in sentenced]\n",
        "    normalized = [sent.replace(\"\\u200cهای\",\"\") for sent in normalized]\n",
        "    normalized = [sent.replace(\"\\u200c\",\" \") for sent in normalized]\n",
        "    # print(f\"normalized:{normalized}\")\n",
        "    lemmed = [str(self.lang_pipe(sent)) for sent in normalized]\n",
        "    # print(f\"lemmed:{lemmed}\")\n",
        "    words_list = [self.word_tokenizer.tokenize(sent) for sent in lemmed]\n",
        "\n",
        "    final_words_list = []\n",
        "    for sent in words_list:\n",
        "      words_second_list = []\n",
        "      tags = self.tagger.tag(sent)\n",
        "      for i in range(len(tags)):\n",
        "        if tags[i][1] != \"V\":\n",
        "          words_second_list.append(tags[i][0])\n",
        "      final_words_list.append(words_second_list)\n",
        "\n",
        "    words_list = list(itertools.chain(*final_words_list))\n",
        "    words_list = [word for word in words_list if word not in self.stop_words]\n",
        "    #lemmed_words_list = [lemmatizer.lemmatize(word) for word in words_list]\n",
        "    # print(lemmed_words_list)\n",
        "\n",
        "    output = dict()\n",
        "    for word in words_list:\n",
        "      if self.is_foreign_word(word):\n",
        "        output[word] = []\n",
        "        add_ind = 0\n",
        "\n",
        "        string = text\n",
        "        while word in string:\n",
        "\n",
        "          begin = string.find(word)\n",
        "          output[word].append((begin+add_ind,add_ind+begin+len(word)))\n",
        "\n",
        "          string = string[begin+len(word):]\n",
        "          add_ind+= begin+len(word)\n",
        "    \n",
        "    return output\n",
        "          "
      ],
      "metadata": {
        "id": "nP6htTpRniww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the run function, is an interactive way to map user_input to the detectors answer\n",
        "def phonetics_run():\n",
        "  detector = phonetics_foreign_word_detector(args)\n",
        "\n",
        "  print(\"enter your sample text.\\nenter an empty string if you want to end the process\")\n",
        "  while True : \n",
        "    text = input()\n",
        "    if text == \"\": \n",
        "      break\n",
        "    \n",
        "    output = detector.detect(text)\n",
        "    print(output)\n"
      ],
      "metadata": {
        "id": "2kHEIVf8pl97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# phonetics_run()"
      ],
      "metadata": {
        "id": "GDGwD8YCpsSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Translate and replace"
      ],
      "metadata": {
        "id": "sLE4BxvSEHIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def change_english_words(text):\n",
        "  from googletrans import Translator\n",
        "  translator = Translator()\n",
        "  detector = foreign_word_detector(args)\n",
        "  output = detector.detect(text)\n",
        "  print(f\"Output: {output}\")\n",
        "  english_text=text\n",
        "  new_text=text\n",
        "  for word in output.keys():\n",
        "    # first_index=output[word][0]\n",
        "    # second_index=output[word][1]\n",
        "    english_form=translator.translate(word).text\n",
        "    persian_translation=translator.translate(english_form, dest='fa').text\n",
        "    persian_translation=persian_translation.split(\"-\")[0]\n",
        "    new_text=new_text.replace(word,persian_translation)\n",
        "  return new_text,english_text"
      ],
      "metadata": {
        "id": "BWa2xvzrNhFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text= \"تسک سختی بود ولی تو کانتریبیوشن خوبی داشتی تنکس .\"\n",
        "text= \"سلام من و فرزندت در گاردنمان همبرگر و استیک خوریدم و آن‌ ها خیلی دلیشز بودند .\"\n",
        "# text=\"من و فرندم نشستیم گیم آف ترونز دیدیم ولی اصلا به گرد پای برکینگ بد هم نمیرسید.\"\n",
        "translation,english_text=change_english_words(text)\n",
        "print(translation)\n",
        "# print(english_text)"
      ],
      "metadata": {
        "id": "qb3F74t0DUMA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}