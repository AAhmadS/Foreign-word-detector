{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMh+rQPLPhTMzR3TBGuhymL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAhmadS/NLP_HW2/blob/main/NLP_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKugniAeQcGt",
        "outputId": "55586e4b-1d46-4dee-f53c-01c6599493c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/NLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRCx-_IwWvxT",
        "outputId": "05134ba7-f974-4830-9eaa-7be7b2e9d68d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1Ikn75GANDVFyb0klJe4-EBQJikt4_Ob_/NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extracting essential data\n",
        "\n"
      ],
      "metadata": {
        "id": "LYf9wqwEYwO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "print(locale.getpreferredencoding())\n",
        "\n",
        "# def getpreferredencoding(do_setlocale = True):\n",
        "#     return \"UTF-8\"\n",
        "# locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "# print(locale.getpreferredencoding())"
      ],
      "metadata": {
        "id": "h1rsrp2ObWmR",
        "outputId": "3c6fd542-5ac3-4a70-81b1-ea11e386b841",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UTF-8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q hazm\n",
        "# !pip install -q transformers\n",
        "!pip install  dadmatools"
      ],
      "metadata": {
        "id": "znLvol8uYzV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b512184-84d6-4563-88cb-d5ee5ff94941"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dadmatools in /usr/local/lib/python3.9/dist-packages (1.5.2)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.9/dist-packages (from dadmatools) (4.5.2)\n",
            "Requirement already satisfied: hyperopt>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (0.2.7)\n",
            "Requirement already satisfied: transformers>=4.9.1 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (4.27.4)\n",
            "Requirement already satisfied: gdown>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (4.6.6)\n",
            "Requirement already satisfied: h5py>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (3.8.0)\n",
            "Requirement already satisfied: folium>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (0.14.0)\n",
            "Requirement already satisfied: sklearn>=0.0 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (0.0.post1)\n",
            "Requirement already satisfied: Deprecated==1.2.6 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (1.2.6)\n",
            "Requirement already satisfied: py7zr>=0.17.2 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (0.20.4)\n",
            "Requirement already satisfied: supar==1.1.2 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (1.1.2)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (1.5.11)\n",
            "Requirement already satisfied: pyconll>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (3.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from dadmatools) (3.3)\n",
            "Requirement already satisfied: html2text in /usr/local/lib/python3.9/dist-packages (from dadmatools) (2020.1.16)\n",
            "Requirement already satisfied: pytorch-transformers>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (1.2.0)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (3.5.1)\n",
            "Requirement already satisfied: tabulate>=0.8.6 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (0.8.10)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (1.13.1+cu116)\n",
            "Requirement already satisfied: gensim>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (4.3.1)\n",
            "Requirement already satisfied: bpemb>=0.3.3 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (0.3.4)\n",
            "Requirement already satisfied: NERDA in /usr/local/lib/python3.9/dist-packages (from dadmatools) (1.0.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.9/dist-packages (from Deprecated==1.2.6->dadmatools) (1.15.0)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.9/dist-packages (from supar==1.1.2->dadmatools) (1.5.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.9/dist-packages (from supar==1.1.2->dadmatools) (0.3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from bpemb>=0.3.3->dadmatools) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from bpemb>=0.3.3->dadmatools) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from bpemb>=0.3.3->dadmatools) (4.65.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from bpemb>=0.3.3->dadmatools) (0.1.97)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from folium>=0.2.1->dadmatools) (0.6.0)\n",
            "Requirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.9/dist-packages (from folium>=0.2.1->dadmatools) (3.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown>=4.3.1->dadmatools) (4.11.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown>=4.3.1->dadmatools) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown>=4.3.1->dadmatools) (3.10.7)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim>=3.6.0->dadmatools) (6.3.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim>=3.6.0->dadmatools) (1.10.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.9/dist-packages (from hyperopt>=0.2.5->dadmatools) (0.10.9.7)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from hyperopt>=0.2.5->dadmatools) (3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from hyperopt>=0.2.5->dadmatools) (0.18.3)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.9/dist-packages (from hyperopt>=0.2.5->dadmatools) (2.2.1)\n",
            "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /usr/local/lib/python3.9/dist-packages (from py7zr>=0.17.2->dadmatools) (1.0.0)\n",
            "Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.9/dist-packages (from py7zr>=0.17.2->dadmatools) (0.2.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from py7zr>=0.17.2->dadmatools) (5.9.4)\n",
            "Requirement already satisfied: inflate64>=0.3.1 in /usr/local/lib/python3.9/dist-packages (from py7zr>=0.17.2->dadmatools) (0.3.1)\n",
            "Requirement already satisfied: brotli>=1.0.9 in /usr/local/lib/python3.9/dist-packages (from py7zr>=0.17.2->dadmatools) (1.0.9)\n",
            "Requirement already satisfied: pyzstd>=0.14.4 in /usr/local/lib/python3.9/dist-packages (from py7zr>=0.17.2->dadmatools) (0.15.4)\n",
            "Requirement already satisfied: pybcj>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from py7zr>=0.17.2->dadmatools) (1.0.1)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.9/dist-packages (from py7zr>=0.17.2->dadmatools) (1.6.7)\n",
            "Requirement already satisfied: pycryptodomex>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from py7zr>=0.17.2->dadmatools) (3.17)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from pytorch-transformers>=1.1.0->dadmatools) (2022.10.31)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.9/dist-packages (from pytorch-transformers>=1.1.0->dadmatools) (1.26.102)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.9/dist-packages (from pytorch-transformers>=1.1.0->dadmatools) (0.0.53)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (67.6.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (2.0.8)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (2.4.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (1.1.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (8.1.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (3.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (0.7.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (3.0.12)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (1.10.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.1->dadmatools) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.9.1->dadmatools) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.9.1->dadmatools) (0.13.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.9.1->dadmatools) (0.13.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from NERDA->dadmatools) (1.4.4)\n",
            "Requirement already satisfied: progressbar in /usr/local/lib/python3.9/dist-packages (from NERDA->dadmatools) (2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2>=2.9->folium>=0.2.1->dadmatools) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->dadmatools) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->dadmatools) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=3.0.0->dadmatools) (8.1.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown>=4.3.1->dadmatools) (2.4)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.102 in /usr/local/lib/python3.9/dist-packages (from boto3->pytorch-transformers>=1.1.0->dadmatools) (1.29.102)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3->pytorch-transformers>=1.1.0->dadmatools) (0.6.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3->pytorch-transformers>=1.1.0->dadmatools) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->NERDA->dadmatools) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->NERDA->dadmatools) (2.8.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (1.7.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->pytorch-transformers>=1.1.0->dadmatools) (1.1.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.9/dist-packages (from stanza->supar==1.1.2->dadmatools) (2.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from stanza->supar==1.1.2->dadmatools) (3.19.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "import hazm\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import itertools\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "vJ1FDzA_Y5VW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('Nlp HW2/extracted-phrases.json','r',encoding='utf-8') as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "b6WyrXTxmsGr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "4de545a6-8069-4dd5-8825-c2a30d222587"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d752a160b50e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Nlp HW2/extracted-phrases.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main code"
      ],
      "metadata": {
        "id": "ezt1J32nymfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Config"
      ],
      "metadata": {
        "id": "uoztZzu6NAYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dadmatools.models.normalizer import Normalizer\n",
        "import dadmatools.pipeline.language as language\n",
        "\n",
        "dadma_normalizer = Normalizer(\n",
        "    full_cleaning=True,\n",
        "    unify_chars=True,\n",
        "    refine_punc_spacing=True,\n",
        "    remove_extra_space=True,\n",
        "    remove_puncs=True,\n",
        "    remove_html=False,\n",
        "    remove_stop_word=True,\n",
        "    replace_email_with=\"<EMAIL>\",\n",
        "    replace_number_with=\"<NUM>\",\n",
        "    replace_url_with=\"\",\n",
        "    replace_mobile_number_with=None,\n",
        "    replace_emoji_with=None,\n",
        "    replace_home_number_with=None,\n",
        ")\n",
        "\n",
        "dadma_normalizer.normalize(\"بیا ای عقل کل با من که بردابرد او بینی\")\n",
        "\n",
        "\n",
        "pips = 'lem' \n",
        "nlp = language.Pipeline(pips)\n",
        "\n",
        "print(nlp.analyze_pipes(pretty=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "gRWJMy0nrWju",
        "outputId": "1f80b3d1-0233-4add-ea96-7ee48ab542a1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-3bbf42169730>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdadmatools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m dadma_normalizer = Normalizer(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfull_cleaning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0munify_chars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/dadmatools/models/normalizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, full_cleaning, unify_chars, refine_punc_spacing, remove_extra_space, remove_puncs, remove_html, remove_stop_word, replace_email_with, replace_number_with, replace_url_with, replace_mobile_number_with, replace_emoji_with, replace_home_number_with)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_puncs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_puncs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_stop_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stop_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTOPWORDS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'stopwords-fa.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'،؟'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfull_cleaning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/encodings/ascii.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascii_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xd8 in position 15: ordinal not in range(128)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [hazm.Lemmatizer().lemmatize(word[0]) for word in tqdm(hazm.words_list())]\n",
        "with open(\"corpus.txt\",\"wb\") as fle:\n",
        "  pickle.dump(corpus,fle)"
      ],
      "metadata": {
        "id": "_9XeR8XowdUh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a75512-b37a-4045-82e2-1c7feba7cf44"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 35717/35717 [00:00<00:00, 1407691.68it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"corpus.txt\",\"rb\") as file:\n",
        "  corpus = pickle.load(file)"
      ],
      "metadata": {
        "id": "BfqmVUF5ZN-z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'corpus':corpus,\n",
        "    'pipe':nlp,\n",
        "    'normalizer':dadma_normalizer,\n",
        "    'translator':None,\n",
        "    'word_tokenizer':hazm.WordTokenizer(),\n",
        "    'sent_tokenizer':hazm.SentenceTokenizer()\n",
        "}"
      ],
      "metadata": {
        "id": "qP0mg4wDOMIm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class foreign_word_detector():\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      args\n",
        "      ):\n",
        "    self.corpus = args['corpus']\n",
        "    self.lang_pipe = args['pipe']\n",
        "    self.normalizer = args['normalizer']\n",
        "    self.translator = args['translator']\n",
        "    self.word_tokenizer = args['word_tokenizer']\n",
        "    self.sent_tokenizer = args['sent_tokenizer']\n",
        "\n",
        "  def detect(self, text):\n",
        "    sentenced = self.sent_tokenizer.tokenize(text)\n",
        "    lemmed = [str(self.lang_pipe(sent)) for sent in sentenced]\n",
        "    normalized = [self.normalizer.normalize(sent) for sent in lemmed]\n",
        "    words_list = [self.word_tokenizer.tokenize(sent) for sent in normalized]\n",
        "    words_list = list(itertools.chain(*words_list))\n",
        "\n",
        "    output = dict()\n",
        "    out = []\n",
        "    for word in words_list:\n",
        "      if word not in self.corpus:\n",
        "        out.append(word)\n",
        "        begin = text.find(word)\n",
        "        output[word] = [begin,begin+len(word)]\n",
        "    \n",
        "    return out, output\n"
      ],
      "metadata": {
        "id": "abisr-pQyq1p"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "  detector = foreign_word_detector(args)\n",
        "\n",
        "  text = \"\"\n",
        "  print(\"enter your sample text.\\nenter an empty string if you want to end the process\")\n",
        "  while True : \n",
        "    text = input()\n",
        "    if text == \"\": \n",
        "      break\n",
        "    \n",
        "    out, output = detector.detect(text)\n",
        "    print(out)\n",
        "    print(output)\n"
      ],
      "metadata": {
        "id": "gFnE-4jRs3Hp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvJUGGdQw1mH",
        "outputId": "8ec19560-20ed-4ace-d56b-cc4c3482f998"
      },
      "execution_count": 28,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter your sample text.\n",
            "enter an empty string if you want to end the process\n",
            "سیستم کامپیوترم خراب شده است.\n",
            "['سیستم', 'کامپیوتر']\n",
            "{'سیستم': [0, 5], 'کامپیوتر': [6, 14]}\n",
            "امروز تسک خیلی هاردی داشتیم اما تو موفق شدی کانتریبیوشن خوبی داشته باشی؛ تنکس.\n",
            "['تسک', 'کانتریبیوشن', 'باشی', '؛', 'تنکس']\n",
            "{'تسک': [6, 9], 'کانتریبیوشن': [44, 55], 'باشی': [67, 71], '؛': [71, 72], 'تنکس': [73, 77]}\n",
            "من و علی در گاردنمان برگر خوردیم.\n",
            "['گاردن', 'خوردیم']\n",
            "{'گاردن': [12, 17], 'خوردیم': [26, 32]}\n",
            "او بسیار آدم کولی بود\n",
            "[]\n",
            "{}\n",
            "او بسیار آدم کولی بود.\n",
            "[]\n",
            "{}\n",
            "امروز تایم ندارم باشد برای فردا.\n",
            "['تایم']\n",
            "{'تایم': [6, 10]}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem is: using stemmer, we get lots of errors; while using lemmatizer, pronouns are not ommited from the word.\n",
        "\n",
        "\n",
        "one way to deal with it would be to apply stemming right after dealing with the word origin.\n",
        "\n",
        "another way would be to find a way to work with the tokenizer introduced in the next cell, since it gives us specific features."
      ],
      "metadata": {
        "id": "4ftp21Xf7JL4"
      }
    }
  ]
}