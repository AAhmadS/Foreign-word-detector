{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAhmadS/NLP_HW2/blob/main/NlpHw2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKugniAeQcGt",
        "outputId": "2db070ee-5ba5-4f46-e3cc-244cfb828341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/NLP/Nlp HW2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRCx-_IwWvxT",
        "outputId": "fc3c00f5-fad8-4690-a0ff-e587a105b3dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/NLP/Nlp HW2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extracting essential data\n",
        "\n"
      ],
      "metadata": {
        "id": "LYf9wqwEYwO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import locale\n",
        "# def getpreferredencoding(do_setlocale = True):\n",
        "#     return \"UTF-8\"\n",
        "# locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "n2iCwQ7T3uBu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==3.1.0a0\n",
        "!pip install -q hazm\n",
        "!pip install dadmatools \n",
        "# !pip install git+https://github.com/Dadmatech/dadmatools.git"
      ],
      "metadata": {
        "id": "znLvol8uYzV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cce4dd80-8c84-4642-b3e5-2048de96bc9c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.12.7)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chardet==3.*\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 KB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting idna==2.*\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hstspreload\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16368 sha256=eb51237bb7110e300f642d5290bebb4c8027f5c64710c047db6ac70241465cdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/e1/6c/5137bc3f35aa130deea71575e165cc4f4f0680a88f3d90a636\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, sniffio, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 4.0.0\n",
            "    Uninstalling chardet-4.0.0:\n",
            "      Successfully uninstalled chardet-4.0.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "Successfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0 sniffio-1.3.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.7/316.7 KB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 KB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dadmatools\n",
            "  Downloading dadmatools-1.5.2-py3-none-any.whl (862 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.6/862.6 KB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from dadmatools) (3.3)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting sklearn>=0.0\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gdown>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (4.6.6)\n",
            "Requirement already satisfied: tabulate>=0.8.6 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (0.8.10)\n",
            "Collecting pytorch-transformers>=1.1.0\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 KB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gensim>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (4.3.1)\n",
            "Collecting NERDA\n",
            "  Downloading NERDA-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting bpemb>=0.3.3\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Collecting pyconll>=3.1.0\n",
            "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Collecting py7zr>=0.17.2\n",
            "  Downloading py7zr-0.20.4-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (3.5.1)\n",
            "Collecting supar==1.1.2\n",
            "  Downloading supar-1.1.2-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated==1.2.6\n",
            "  Downloading Deprecated-1.2.6-py2.py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: folium>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (0.14.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (2.0.0+cu118)\n",
            "Requirement already satisfied: hyperopt>=0.2.5 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (0.2.7)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 KB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from dadmatools) (3.8.0)\n",
            "Collecting html2text\n",
            "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Collecting transformers>=4.9.1\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting conllu\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.9/dist-packages (from Deprecated==1.2.6->dadmatools) (1.14.1)\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.5.0-py3-none-any.whl (802 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.5/802.5 KB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from bpemb>=0.3.3->dadmatools) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from bpemb>=0.3.3->dadmatools) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from bpemb>=0.3.3->dadmatools) (4.65.0)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.9/dist-packages (from folium>=0.2.1->dadmatools) (3.1.2)\n",
            "Requirement already satisfied: branca>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from folium>=0.2.1->dadmatools) (0.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown>=4.3.1->dadmatools) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown>=4.3.1->dadmatools) (4.11.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown>=4.3.1->dadmatools) (3.10.7)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim>=3.6.0->dadmatools) (6.3.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim>=3.6.0->dadmatools) (1.10.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.9/dist-packages (from hyperopt>=0.2.5->dadmatools) (2.2.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.9/dist-packages (from hyperopt>=0.2.5->dadmatools) (0.10.9.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from hyperopt>=0.2.5->dadmatools) (0.18.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from hyperopt>=0.2.5->dadmatools) (3.0)\n",
            "Collecting multivolumefile>=0.2.3\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting texttable\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.6.6\n",
            "  Downloading pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inflate64>=0.3.1\n",
            "  Downloading inflate64-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli>=1.0.9\n",
            "  Downloading Brotli-1.0.9-cp39-cp39-manylinux1_x86_64.whl (357 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 KB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from py7zr>=0.17.2->dadmatools) (5.9.4)\n",
            "Collecting pyppmd<1.1.0,>=0.18.1\n",
            "  Downloading pyppmd-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 KB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybcj>=0.6.0\n",
            "  Downloading pybcj-1.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.14.4\n",
            "  Downloading pyzstd-0.15.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (384 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.0/384.0 KB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from pytorch-transformers>=1.1.0->dadmatools) (2022.10.31)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.104-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 KB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (2.4.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (3.0.12)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (67.6.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (1.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (1.10.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (1.0.9)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (0.7.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (23.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.0.0->dadmatools) (8.1.9)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.1->dadmatools) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.1->dadmatools) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.1->dadmatools) (4.5.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.1->dadmatools) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.1->dadmatools) (3.25.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.9.1->dadmatools) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting progressbar\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from NERDA->dadmatools) (1.4.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2>=2.9->folium>=0.2.1->dadmatools) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (2.0.12)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->dadmatools) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.0.0->dadmatools) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=3.0.0->dadmatools) (8.1.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown>=4.3.1->dadmatools) (2.4)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.30.0,>=1.29.104\n",
            "  Downloading botocore-1.29.104-py3-none-any.whl (10.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->NERDA->dadmatools) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->NERDA->dadmatools) (2.8.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests->bpemb>=0.3.3->dadmatools) (1.7.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from sacremoses->pytorch-transformers>=1.1.0->dadmatools) (1.1.1)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from stanza->supar==1.1.2->dadmatools) (3.20.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.7.1->dadmatools) (1.3.0)\n",
            "Building wheels for collected packages: sklearn, progressbar, sacremoses, emoji\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2955 sha256=a113e32b99b560e45a551db8f78a7fca7bc8739dcb8a30e2dafbb39aedd79a8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/e0/3d/9d0c2020c44a519b9f02ab4fa6d2a4a996c98d79ab2f569fa1\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12080 sha256=1b413be9371623cf0b04374404962c8f6c88351e3aa41aec3e87cb8cc4c93e81\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/d9/89/a3f31c76ff6d51dc3b1575628f59afe59e4ceae3f2748cd7ad\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=39e305a5b13c921406ac7981cecd391186d42dee57ad5f65443175fb7513e1c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=21efb314356d945520865dbbf973d2a15e4b3d8a461f7890ce2251df08606c49\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/b8/0f/f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n",
            "Successfully built sklearn progressbar sacremoses emoji\n",
            "Installing collected packages: tokenizers, tf-estimator-nightly, texttable, sklearn, sentencepiece, progressbar, brotli, segtok, sacremoses, pyzstd, pyppmd, pycryptodomex, pyconll, pybcj, multivolumefile, jmespath, inflate64, html2text, emoji, dill, Deprecated, conllu, py7zr, huggingface-hub, botocore, transformers, s3transfer, bpemb, boto3, stanza, supar, pytorch-transformers, NERDA, dadmatools\n",
            "Successfully installed Deprecated-1.2.6 NERDA-1.0.0 boto3-1.26.104 botocore-1.29.104 bpemb-0.3.4 brotli-1.0.9 conllu-4.5.2 dadmatools-1.5.2 dill-0.3.6 emoji-2.2.0 html2text-2020.1.16 huggingface-hub-0.13.3 inflate64-0.3.1 jmespath-1.0.1 multivolumefile-0.2.3 progressbar-2.5 py7zr-0.20.4 pybcj-1.0.1 pyconll-3.1.0 pycryptodomex-3.17 pyppmd-1.0.0 pytorch-transformers-1.2.0 pyzstd-0.15.4 s3transfer-0.6.0 sacremoses-0.0.53 segtok-1.5.11 sentencepiece-0.1.97 sklearn-0.0.post1 stanza-1.5.0 supar-1.1.2 texttable-1.6.7 tf-estimator-nightly-2.8.0.dev2021122109 tokenizers-0.13.2 transformers-4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "import hazm\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import itertools\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "vJ1FDzA_Y5VW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('Nlp HW2/extracted-phrases.json','r',encoding='utf-8') as file:\n",
        "#     data = json.load(file)"
      ],
      "metadata": {
        "id": "b6WyrXTxmsGr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# words = []\n",
        "# phonics_list = []\n",
        "# dict_file = open('cmudict.dict', 'r')\n",
        "# with dict_file as f:\n",
        "#     phonics = [line.rstrip('\\n') for line in f]\n",
        "#     for p in phonics:\n",
        "#         x = p.split(' ')\n",
        "#         words.append(x[0])\n",
        "#         phonics_list.append(' '.join(x[1:]))"
      ],
      "metadata": {
        "id": "_kzbejEP6GP7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# phons_uniqe = list(set(itertools.chain(*[ph.split(\" \") for ph in phonics_list])))\n",
        "# phons_as_list = [ph.split(\" \") for ph in phonics_list]"
      ],
      "metadata": {
        "id": "zulC4Adu6upO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"phons_exchange.txt\",\"rb\") as file:\n",
        "#   phons_dict = pickle.load(file)"
      ],
      "metadata": {
        "id": "vtwOxqizjC-X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# phons_dict[\"NG\"] = (\"ن\")\n",
        "# phons_dict[\"TH\"] = (\"ت\")"
      ],
      "metadata": {
        "id": "6_UTCNuVjWlr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# persianized_list = []\n",
        "# persianized = []\n",
        "# for x in phons_as_list:\n",
        "#   word_list = []\n",
        "#   count=0\n",
        "#   for ph in x:\n",
        "#     count+=1\n",
        "#     if len(phons_dict[ph])>1:\n",
        "#       if count==1:\n",
        "#         word_list.append(phons_dict[ph][0])\n",
        "#       else:\n",
        "#         word_list.append(phons_dict[ph][1])\n",
        "#     else:\n",
        "#       word_list.append(phons_dict[ph][0])\n",
        "\n",
        "#   persianized_list.append(word_list)\n",
        "#   persianized.append(''.join(word_list))"
      ],
      "metadata": {
        "id": "TZQlhU9GjSJi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# words[21000:21010]"
      ],
      "metadata": {
        "id": "5_pSldPjknPo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# persianized[21000:21010]"
      ],
      "metadata": {
        "id": "4mKUNAPmkdt9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# listt = ['contribution','thanks','computer','system','task','time','garden','burger']\n",
        "\n",
        "# for i in range(len(words)):\n",
        "#   if words[i] in listt:\n",
        "#     print(words[i])\n",
        "#     print(phons_as_list[i])\n",
        "#     print(i)\n",
        "#     print(persianized[i])\n",
        "#     print(\"-----------------\")"
      ],
      "metadata": {
        "id": "Pm2B0nXskxX8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main code"
      ],
      "metadata": {
        "id": "ezt1J32nymfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Config"
      ],
      "metadata": {
        "id": "uoztZzu6NAYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dadmatools.models.normalizer import Normalizer\n",
        "\n",
        "dadma_normalizer = Normalizer(\n",
        "    full_cleaning=True,\n",
        "    unify_chars=True,\n",
        "    refine_punc_spacing=True,\n",
        "    remove_extra_space=True,\n",
        "    remove_puncs=True,\n",
        "    remove_html=True,\n",
        "    remove_stop_word=True,\n",
        "    replace_email_with=\"\",\n",
        "    replace_number_with=\"\",\n",
        "    replace_url_with=\"\",\n",
        "    replace_mobile_number_with=None,\n",
        "    replace_emoji_with=None,\n",
        "    replace_home_number_with=None,\n",
        ")\n",
        "\n",
        "dadma_normalizer.normalize(\"بیا ای عقل کل با من که بردابرد او بینی\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gRWJMy0nrWju",
        "outputId": "1ba52335-a281-43d9-fe4c-55335936c054"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'عقل بردابرد بینی'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = hazm.Lemmatizer()\n",
        "stemmer = hazm.Stemmer()"
      ],
      "metadata": {
        "id": "LZ0cqll7Zwdu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dadmatools.pipeline.language as language\n",
        "\n",
        "pips = 'lem' \n",
        "nlp = language.Pipeline(pips)\n",
        "\n",
        "print(nlp.analyze_pipes(pretty=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ShvxXYdXJ0Y",
        "outputId": "025b4818-1fde-47fc-f6e6-e594828f0279"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading file /root/.pernlp/fa_tokenizer.pt: : 639kB [00:01, 420kB/s]                           \n",
            "Downloading file /root/.pernlp/fa_mwt.pt: : 721kB [00:01, 453kB/s]                         \n",
            "Downloading file /root/.pernlp/fa_lemmatizer.pt: : 4.69MB [00:02, 1.94MB/s]                          \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================= Pipeline Overview =============================\u001b[0m\n",
            "\n",
            "#   Component   Assigns       Requires   Scores   Retokenizes\n",
            "-   ---------   -----------   --------   ------   -----------\n",
            "0   tokenizer                                     True       \n",
            "                                                             \n",
            "1   lemmatize   token.lemma                       False      \n",
            "\n",
            "\u001b[38;5;2m✔ No problems found.\u001b[0m\n",
            "{'summary': {'tokenizer': {'assigns': [], 'requires': [], 'scores': [], 'retokenizes': True}, 'lemmatize': {'assigns': ['token.lemma'], 'requires': [], 'scores': [], 'retokenizes': False}}, 'problems': {'tokenizer': [], 'lemmatize': []}, 'attrs': {'token.lemma': {'assigns': ['lemmatize'], 'requires': []}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"corpus.txt\",\"rb\") as f:\n",
        "  corpo = pickle.load(f)\n",
        "\n",
        "corpo"
      ],
      "metadata": {
        "id": "wtvAazXLhkjG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "747762f8-3ade-462a-dc56-d8de3e7756e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['آئرودینامیک',\n",
              " 'آئرومتر',\n",
              " 'آئورت',\n",
              " 'آبا',\n",
              " 'آباد',\n",
              " 'آباد',\n",
              " 'آباد',\n",
              " 'آباد',\n",
              " 'آباد',\n",
              " 'آباد',\n",
              " 'آباد',\n",
              " 'آباد',\n",
              " 'آبادانی',\n",
              " 'آبادانی',\n",
              " 'آبادانی',\n",
              " 'آبادبوم',\n",
              " 'آبادگر',\n",
              " 'آباد',\n",
              " 'آباد',\n",
              " 'آباد جای',\n",
              " 'آباد کرد',\n",
              " 'آباد',\n",
              " 'آبار',\n",
              " 'آبال',\n",
              " 'آبان',\n",
              " 'آبان',\n",
              " 'آبان',\n",
              " 'آبانگان',\n",
              " 'آباژور',\n",
              " 'آباژور',\n",
              " 'آبتابه',\n",
              " 'آبجو',\n",
              " 'آبجی',\n",
              " 'آبخست',\n",
              " 'آبخو',\n",
              " 'آبخور',\n",
              " 'آبخور',\n",
              " 'آبخور',\n",
              " 'آبخور',\n",
              " 'آبخور',\n",
              " 'آبخورد',\n",
              " 'آبخورد',\n",
              " 'آبخورد',\n",
              " 'آبخورد',\n",
              " 'آبخوری',\n",
              " 'آبخوری',\n",
              " 'آبخوری',\n",
              " 'آبخوری',\n",
              " 'آبخوست',\n",
              " 'آبخیز',\n",
              " 'آبخیز',\n",
              " 'آبخیز',\n",
              " 'آبخیز',\n",
              " 'آبخیز',\n",
              " 'آبدار',\n",
              " 'آبدار',\n",
              " 'آبدار',\n",
              " 'آبدار',\n",
              " 'آبدار',\n",
              " 'آبدار',\n",
              " 'آبدار',\n",
              " 'آبدار',\n",
              " 'آبدارخانه',\n",
              " 'آبدارک',\n",
              " 'آبدان',\n",
              " 'آبدان',\n",
              " 'آبدان',\n",
              " 'آبدان',\n",
              " 'آبدزدک',\n",
              " 'آبدزدک',\n",
              " 'آبدندان',\n",
              " 'آبدندان',\n",
              " 'آبدندان',\n",
              " 'آبدندان',\n",
              " 'آبده',\n",
              " 'آبده',\n",
              " 'آبراهه',\n",
              " 'آبراهه',\n",
              " 'آبرفت',\n",
              " 'آبرفت',\n",
              " 'آبرو',\n",
              " 'آبرو',\n",
              " 'آبرو',\n",
              " 'آبرو',\n",
              " 'آبروخواه',\n",
              " 'آبرودار',\n",
              " 'آبروداری',\n",
              " 'آبرومند',\n",
              " 'آبرومند',\n",
              " 'آبرومند',\n",
              " 'آبرومند',\n",
              " 'آبریز',\n",
              " 'آبریز',\n",
              " 'آبریز',\n",
              " 'آبریز',\n",
              " 'آبریز',\n",
              " 'آبریز',\n",
              " 'آبریزگان',\n",
              " 'آبزن',\n",
              " 'آبزه',\n",
              " 'آبزه',\n",
              " 'آبزی',\n",
              " 'آبسال',\n",
              " 'آبسال',\n",
              " 'آبسال',\n",
              " 'آبست',\n",
              " 'آبستان',\n",
              " 'آبستن',\n",
              " 'آبستن',\n",
              " 'آبستنی',\n",
              " 'آبستنی',\n",
              " 'آبسته',\n",
              " 'آبست',\n",
              " 'آبسه',\n",
              " 'آبشار',\n",
              " 'آبشار',\n",
              " 'آبشامه',\n",
              " 'آبشتنگاه',\n",
              " 'آبشتنگاه',\n",
              " 'آبشخور',\n",
              " 'آبشخور',\n",
              " 'آبشخور',\n",
              " 'آبغوره',\n",
              " 'آبفت',\n",
              " 'آبله',\n",
              " 'آبله',\n",
              " 'آبله رو',\n",
              " 'آبله مرغان',\n",
              " 'آبله کوب',\n",
              " 'آبله کوبان',\n",
              " 'آبله کوبی',\n",
              " 'آبلوج',\n",
              " 'آبلوج',\n",
              " 'آبناک',\n",
              " 'آبنوس',\n",
              " 'آبنوس',\n",
              " 'آبنوس',\n",
              " 'آبه',\n",
              " 'آبو',\n",
              " 'آبوند',\n",
              " 'آبوند',\n",
              " 'آبوند',\n",
              " 'آبونمان',\n",
              " 'آبچین',\n",
              " 'آبچین',\n",
              " 'آبک',\n",
              " 'آبک',\n",
              " 'آبکامه',\n",
              " 'آبکامه',\n",
              " 'آبکش',\n",
              " 'آبکش',\n",
              " 'آبکش',\n",
              " 'آبکش',\n",
              " 'آبکش',\n",
              " 'آبکند',\n",
              " 'آبکند',\n",
              " 'آبکند',\n",
              " 'آبکی',\n",
              " 'آبکی',\n",
              " 'آبکی',\n",
              " 'آبکی',\n",
              " 'آبکی',\n",
              " 'آبکی',\n",
              " 'آبگاه',\n",
              " 'آبگاه',\n",
              " 'آبگاه',\n",
              " 'آبگاه',\n",
              " 'آبگذر',\n",
              " 'آبگرد',\n",
              " 'آبگردان',\n",
              " 'آبگوشت',\n",
              " 'آبگون',\n",
              " 'آبگون',\n",
              " 'آبگون',\n",
              " 'آبگون',\n",
              " 'آبگون',\n",
              " 'آبگون',\n",
              " 'آبگون',\n",
              " 'آبگون',\n",
              " 'آبگیر',\n",
              " 'آبگیر',\n",
              " 'آبگیر',\n",
              " 'آبگیر',\n",
              " 'آبگیر',\n",
              " 'آبگینه',\n",
              " 'آبگینه',\n",
              " 'آبگینه',\n",
              " 'آبگینه',\n",
              " 'آبگینه گر',\n",
              " 'آب',\n",
              " 'آب',\n",
              " 'آب',\n",
              " 'آب',\n",
              " 'آب',\n",
              " 'آب',\n",
              " 'آب',\n",
              " 'آبیار',\n",
              " 'آبیاری',\n",
              " 'آبیاری',\n",
              " 'آب',\n",
              " 'آب آشنا',\n",
              " 'آب آهنگ',\n",
              " 'آب انبار',\n",
              " 'آب انبار',\n",
              " 'آب باز',\n",
              " 'آب باز',\n",
              " 'آب بازان',\n",
              " 'آب بازی',\n",
              " 'آب بازی',\n",
              " 'آب بند',\n",
              " 'آب بند',\n",
              " 'آب بندی',\n",
              " 'آب بندی',\n",
              " 'آب بندی',\n",
              " 'آب بها',\n",
              " 'آب بها',\n",
              " 'آب تره',\n",
              " 'آب تل',\n",
              " 'آب تنی',\n",
              " 'آب تنی',\n",
              " 'آب خانه',\n",
              " 'آب خانه',\n",
              " 'آب خسب',\n",
              " 'آب خست',\n",
              " 'آب خشک کن',\n",
              " 'آب خوار',\n",
              " 'آب خواره',\n",
              " 'آب داده',\n",
              " 'آب درمانی',\n",
              " 'آب دست',\n",
              " 'آب دست',\n",
              " 'آب دست',\n",
              " 'آب دست',\n",
              " 'آب دست',\n",
              " 'آب دستان',\n",
              " 'آب دستی',\n",
              " 'آب دوغ',\n",
              " 'آب دوغ خیار',\n",
              " 'آب دیده',\n",
              " 'آب دیده',\n",
              " 'آب رنگ',\n",
              " 'آب رنگ',\n",
              " 'آب رو',\n",
              " 'آب رو',\n",
              " 'آب ریزه',\n",
              " 'آب ریزه',\n",
              " 'آب زیرکاه',\n",
              " 'آب سنج',\n",
              " 'آب سنگ',\n",
              " 'آب سوار',\n",
              " 'آب شش',\n",
              " 'آب شناس',\n",
              " 'آب شناس',\n",
              " 'آب شناس',\n",
              " 'آب شنگولی',\n",
              " 'آب شویی',\n",
              " 'آب شیب',\n",
              " 'آب شیب',\n",
              " 'آب فشان',\n",
              " 'آب لمبو',\n",
              " 'آب لیمو',\n",
              " 'آب مروارید',\n",
              " 'آب معدنی',\n",
              " 'آب مقطر',\n",
              " 'آب نبات',\n",
              " 'آب نما',\n",
              " 'آب نما',\n",
              " 'آب نما',\n",
              " 'آب نما',\n",
              " 'آب نورد',\n",
              " 'آب نگاری',\n",
              " 'آب ورزی',\n",
              " 'آب ورنگ',\n",
              " 'آب ورنگ',\n",
              " 'آب ورنگ',\n",
              " 'آب پاشان',\n",
              " 'آب پاشی',\n",
              " 'آب پخشان',\n",
              " 'آب پز',\n",
              " 'آب چر',\n",
              " 'آب چر',\n",
              " 'آب کار',\n",
              " 'آب کار',\n",
              " 'آب کار',\n",
              " 'آب کار',\n",
              " 'آب کاری',\n",
              " 'آب کاری',\n",
              " 'آب کاری',\n",
              " 'آب کور',\n",
              " 'آب کور',\n",
              " 'آب کور',\n",
              " 'آب گردش',\n",
              " 'آب گردش',\n",
              " 'آب گرم کن',\n",
              " 'آب گز',\n",
              " 'آتاش',\n",
              " 'آتاشه',\n",
              " 'آتروپین',\n",
              " 'آتریاد',\n",
              " 'آتش',\n",
              " 'آتش',\n",
              " 'آتش',\n",
              " 'آتش',\n",
              " 'آتش',\n",
              " 'آتش',\n",
              " 'آتشبار',\n",
              " 'آتشبار',\n",
              " 'آتشبار',\n",
              " 'آتشبار',\n",
              " 'آتشبان',\n",
              " 'آتشدان',\n",
              " 'آتشدان',\n",
              " 'آتشدان',\n",
              " 'آتشدان',\n",
              " 'آتشناک',\n",
              " 'آتشه',\n",
              " 'آتشک',\n",
              " 'آتشکار',\n",
              " 'آتشکار',\n",
              " 'آتشکده',\n",
              " 'آتشگاه',\n",
              " 'آتشگاه',\n",
              " 'آتشگاه',\n",
              " 'آتشگون',\n",
              " 'آتشگیره',\n",
              " 'آتش',\n",
              " 'آتش',\n",
              " 'آتش',\n",
              " 'آتشین',\n",
              " 'آتشین',\n",
              " 'آتشین',\n",
              " 'آتشین',\n",
              " 'آتشین',\n",
              " 'آتشین',\n",
              " 'آتشی مزاج',\n",
              " 'آتشی مزاج',\n",
              " 'آتش افروز',\n",
              " 'آتش افروز',\n",
              " 'آتش افروز',\n",
              " 'آتش افروز',\n",
              " 'آتش افروزی',\n",
              " 'آتش افروزی',\n",
              " 'آتش انداز',\n",
              " 'آتش انداز',\n",
              " 'آتش انگیز',\n",
              " 'آتش انگیز',\n",
              " 'آتش بازی',\n",
              " 'آتش بازی',\n",
              " 'آتش برگ',\n",
              " 'آتش بس',\n",
              " 'آتش تاب',\n",
              " 'آتش تاب',\n",
              " 'آتش خانه',\n",
              " 'آتش خانه',\n",
              " 'آتش خانه',\n",
              " 'آتش خانه',\n",
              " 'آتش خانه',\n",
              " 'آتش خای',\n",
              " 'آتش خو',\n",
              " 'آتش خوار',\n",
              " 'آتش خوار',\n",
              " 'آتش خوار',\n",
              " 'آتش خوار',\n",
              " 'آتش خوار',\n",
              " 'آتش دست',\n",
              " 'آتش رنگ',\n",
              " 'آتش زا',\n",
              " 'آتش زا',\n",
              " 'آتش زبان',\n",
              " 'آتش زنه',\n",
              " 'آتش زنه',\n",
              " 'آتش سرخ کن',\n",
              " 'آتش سری',\n",
              " 'آتش سوز',\n",
              " 'آتش سوز',\n",
              " 'آتش سوز',\n",
              " 'آتش سوزی',\n",
              " 'آتش فام',\n",
              " 'آتش فروز',\n",
              " 'آتش فشان',\n",
              " 'آتش فشان',\n",
              " 'آتش فشان',\n",
              " 'آتش فشانی',\n",
              " 'آتش فشانی',\n",
              " 'آتش مزاج',\n",
              " 'آتش مزاج',\n",
              " 'آتش نشان',\n",
              " 'آتش نشان',\n",
              " 'آتش نشان',\n",
              " 'آتش نشانی',\n",
              " 'آتش نشانی',\n",
              " 'آتش نشانی',\n",
              " 'آتش نفس',\n",
              " 'آتش نفس',\n",
              " 'آتش نهاد',\n",
              " 'آتش وار',\n",
              " 'آتش پا',\n",
              " 'آتش پا',\n",
              " 'آتش پاره',\n",
              " 'آتش پاره',\n",
              " 'آتش پاره',\n",
              " 'آتش پاره',\n",
              " 'آتش پرست',\n",
              " 'آتش پرست',\n",
              " 'آتش پرستی',\n",
              " 'آتش چرخان',\n",
              " 'آتش گردان',\n",
              " 'آتل',\n",
              " 'آتلیه',\n",
              " 'آتمسفر',\n",
              " 'آتو',\n",
              " 'آتو',\n",
              " 'آتون',\n",
              " 'آتی',\n",
              " 'آتیه',\n",
              " 'آثار',\n",
              " 'آثام',\n",
              " 'آثم',\n",
              " 'آجال',\n",
              " 'آجام',\n",
              " 'آجدن',\n",
              " 'آجر',\n",
              " 'آجل',\n",
              " 'آجل',\n",
              " 'آجل',\n",
              " 'آجل',\n",
              " 'آجودان',\n",
              " 'آجیدن',\n",
              " 'آجیدن',\n",
              " 'آجیده',\n",
              " 'آجیده',\n",
              " 'آجیده',\n",
              " 'آجیل',\n",
              " 'آجیل خوری',\n",
              " 'آجیل فروش',\n",
              " 'آجین',\n",
              " 'آجین',\n",
              " 'آحاد',\n",
              " 'آحاد',\n",
              " 'آحاد',\n",
              " 'آخال',\n",
              " 'آخال',\n",
              " 'آخال',\n",
              " 'آختن',\n",
              " 'آخته',\n",
              " 'آخته',\n",
              " 'آخته',\n",
              " 'آخر',\n",
              " 'آخر',\n",
              " 'آخر',\n",
              " 'آخر',\n",
              " 'آخرالامر',\n",
              " 'آخرالزمان',\n",
              " 'آخربین',\n",
              " 'آخرت',\n",
              " 'آخرت',\n",
              " 'آخردست',\n",
              " 'آخردست',\n",
              " 'آخردست',\n",
              " 'آخردست',\n",
              " 'آخرک',\n",
              " 'آخر',\n",
              " 'آخرین',\n",
              " 'آخر',\n",
              " 'آخ',\n",
              " 'آخشیج',\n",
              " 'آخشیج',\n",
              " 'آخور',\n",
              " 'آخور',\n",
              " 'آخورسالار',\n",
              " 'آخورچی',\n",
              " 'آخوند',\n",
              " 'آخوند',\n",
              " 'آخوند',\n",
              " 'آخوندک',\n",
              " 'آداب',\n",
              " 'آداب',\n",
              " 'آداب دان',\n",
              " 'آداش',\n",
              " 'آدامس',\n",
              " 'آداک',\n",
              " 'آدخ',\n",
              " 'آدخ',\n",
              " 'آدرس',\n",
              " 'آدرس',\n",
              " 'آدرم',\n",
              " 'آدرنال',\n",
              " 'آدرنالین',\n",
              " 'آدرنگ',\n",
              " 'آدم',\n",
              " 'آدم',\n",
              " 'آدم',\n",
              " 'آدم',\n",
              " 'آدمک',\n",
              " 'آدمک',\n",
              " 'آدم',\n",
              " 'آدمیت',\n",
              " 'آدمیت',\n",
              " 'آدمیرال',\n",
              " 'آدمیزاد',\n",
              " 'آدمیزاده',\n",
              " 'آدمی خوار',\n",
              " 'آدمی سیرتی',\n",
              " 'آدمی کش',\n",
              " 'آدمی کش',\n",
              " 'آدم خوار',\n",
              " 'آدم خوار',\n",
              " 'آدم نمایان',\n",
              " 'آدنوئید',\n",
              " 'آدنوئید',\n",
              " 'آدنیس',\n",
              " 'آده',\n",
              " 'آدیش',\n",
              " 'آدینده',\n",
              " 'آدینه',\n",
              " 'آذار',\n",
              " 'آذاراقی',\n",
              " 'آذان',\n",
              " 'آذان الفار',\n",
              " 'آذر',\n",
              " 'آذر',\n",
              " 'آذر',\n",
              " 'آذربو',\n",
              " 'آذرجشن',\n",
              " 'آذرخش',\n",
              " 'آذرروز',\n",
              " 'آذرسنج',\n",
              " 'آذرفزا',\n",
              " 'آذرنگ',\n",
              " 'آذرنگ',\n",
              " 'آذرنگ',\n",
              " 'آذرنگ',\n",
              " 'آذرنگی',\n",
              " 'آذرپرست',\n",
              " 'آذرپرست',\n",
              " 'آذرگان',\n",
              " 'آذرگشسب',\n",
              " 'آذرگون',\n",
              " 'آذرگون',\n",
              " 'آذرگون',\n",
              " 'آذر',\n",
              " 'آذر',\n",
              " 'آذر',\n",
              " 'آذر',\n",
              " 'آذر',\n",
              " 'آذر',\n",
              " 'آذر',\n",
              " 'آذرین',\n",
              " 'آذریون',\n",
              " 'آذوقه',\n",
              " 'آذوقه',\n",
              " 'آذین',\n",
              " 'آذین',\n",
              " 'آذین',\n",
              " 'آذین بندی',\n",
              " 'آرا',\n",
              " 'آرا',\n",
              " 'آرا',\n",
              " 'آراب',\n",
              " 'آراستن',\n",
              " 'آراستن',\n",
              " 'آراستن',\n",
              " 'آراستن',\n",
              " 'آراستن',\n",
              " 'آراسته',\n",
              " 'آراسته',\n",
              " 'آراسته',\n",
              " 'آراستگی',\n",
              " 'آراستگی',\n",
              " 'آراستگی',\n",
              " 'آرام',\n",
              " 'آرام',\n",
              " 'آرام',\n",
              " 'آرام',\n",
              " 'آرام',\n",
              " 'آرام',\n",
              " 'آرام',\n",
              " 'آرام',\n",
              " 'آرام',\n",
              " 'آرام',\n",
              " 'آرامانیدن',\n",
              " 'آرامش',\n",
              " 'آرامش',\n",
              " 'آرامش',\n",
              " 'آرامش',\n",
              " 'آرامش',\n",
              " 'آرامشگاه',\n",
              " 'آرامگاه',\n",
              " 'آرامگاه',\n",
              " 'آرامگاه',\n",
              " 'آرامی',\n",
              " 'آرامی',\n",
              " 'آرامیدن',\n",
              " 'آرام بخش',\n",
              " 'آرام جا',\n",
              " 'آرام جو',\n",
              " 'آرام جو',\n",
              " 'آرام سوز',\n",
              " 'آرام',\n",
              " 'آرایش',\n",
              " 'آرایش',\n",
              " 'آرایش',\n",
              " 'آرایش',\n",
              " 'آرایش',\n",
              " 'آرایشگاه',\n",
              " 'آرایشگر',\n",
              " 'آرایشگر',\n",
              " 'آراینده',\n",
              " 'آراینده',\n",
              " 'آراییدن',\n",
              " 'آرا',\n",
              " 'آرترولوژی',\n",
              " 'آرتروگرافی',\n",
              " 'آرتزین',\n",
              " 'آرتیست',\n",
              " 'آرتیست',\n",
              " 'آرتیست',\n",
              " 'آرتیشو',\n",
              " 'آرتیکل',\n",
              " 'آرخالق',\n",
              " 'آرد',\n",
              " 'آردال',\n",
              " 'آردال',\n",
              " 'آرداله',\n",
              " 'آردبیز',\n",
              " 'آردبیز',\n",
              " 'آردل',\n",
              " 'آردن',\n",
              " 'آردن',\n",
              " 'آرده',\n",
              " 'آردهاله',\n",
              " 'آرد',\n",
              " 'آردینه',\n",
              " 'آردینه',\n",
              " 'آرزو',\n",
              " 'آرزو',\n",
              " 'آرزو',\n",
              " 'آرزو',\n",
              " 'آرزو',\n",
              " 'آرزو',\n",
              " 'آرزوانه',\n",
              " 'آرزوانه',\n",
              " 'آرزوخواه',\n",
              " 'آرزوسنج',\n",
              " 'آرزومند',\n",
              " 'آرزومند',\n",
              " 'آرزومند',\n",
              " 'آرزومندانه',\n",
              " 'آرزومندانه',\n",
              " 'آرزومند',\n",
              " 'آرزومند',\n",
              " 'آرزوناک',\n",
              " 'آرسنیک',\n",
              " 'آرشه',\n",
              " 'آرشیتکت',\n",
              " 'آرشیدوک',\n",
              " 'آرشین',\n",
              " 'آرشیو',\n",
              " 'آرشیو',\n",
              " 'آرشیویست',\n",
              " 'آرغده',\n",
              " 'آرغده',\n",
              " 'آرم',\n",
              " 'آرمان',\n",
              " 'آرمان',\n",
              " 'آرمان',\n",
              " 'آرمان',\n",
              " 'آرمان طلب',\n",
              " 'آرمان گرا',\n",
              " 'آرمده',\n",
              " 'آرمیدن',\n",
              " 'آرمیدن',\n",
              " 'آرمیده',\n",
              " 'آرمیده',\n",
              " 'آرمیده',\n",
              " 'آرمیده',\n",
              " 'آرمیچر',\n",
              " 'آرن',\n",
              " 'آرنج',\n",
              " 'آرنگ',\n",
              " 'آرنگ',\n",
              " 'آرنگ',\n",
              " 'آرواره',\n",
              " 'آروغ',\n",
              " 'آرکئوزوئیک',\n",
              " 'آرکئولوژی',\n",
              " 'آرگن',\n",
              " 'آری',\n",
              " 'آریا',\n",
              " 'آریستوکرات',\n",
              " 'آریستوکرات',\n",
              " 'آریستوکرات',\n",
              " 'آریستوکراسی',\n",
              " 'آریستوکراسی',\n",
              " 'آریغ',\n",
              " 'آریغ',\n",
              " 'آزاد',\n",
              " 'آزاد',\n",
              " 'آزاد',\n",
              " 'آزاد',\n",
              " 'آزاد',\n",
              " 'آزاد',\n",
              " 'آزاد',\n",
              " 'آزاد',\n",
              " 'آزاددرخت',\n",
              " 'آزادزاده',\n",
              " 'آزادسرو',\n",
              " 'آزادسرو',\n",
              " 'آزادماهی',\n",
              " 'آزادمرد',\n",
              " 'آزادمرد',\n",
              " 'آزادمرد',\n",
              " 'آزادمنش',\n",
              " 'آزادمیوه',\n",
              " 'آزاده',\n",
              " 'آزاده',\n",
              " 'آزاده',\n",
              " 'آزاده',\n",
              " 'آزاده',\n",
              " 'آزاده',\n",
              " 'آزاده خو',\n",
              " 'آزاده خو',\n",
              " 'آزاده نژاد',\n",
              " 'آزادوار',\n",
              " 'آزادوار',\n",
              " 'آزادکار',\n",
              " 'آزادکرد',\n",
              " 'آزادگی',\n",
              " 'آزادگی',\n",
              " 'آزادگی',\n",
              " 'آزاد',\n",
              " 'آزاد',\n",
              " 'آزاد',\n",
              " 'آزاد',\n",
              " 'آزاد',\n",
              " 'آزاد',\n",
              " 'آزادی خواه',\n",
              " 'آزادی خواه',\n",
              " 'آزار',\n",
              " 'آزار',\n",
              " 'آزار',\n",
              " 'آزار',\n",
              " 'آزار',\n",
              " 'آزار',\n",
              " 'آزار',\n",
              " 'آزاردن',\n",
              " 'آزار',\n",
              " 'آزارمند',\n",
              " 'آزارمند',\n",
              " 'آزارنده',\n",
              " 'آزارگر',\n",
              " 'آزال',\n",
              " 'آزالیا',\n",
              " 'آزجو',\n",
              " 'آزخ',\n",
              " 'آزدن',\n",
              " 'آزردن',\n",
              " 'آزردن',\n",
              " 'آزرده',\n",
              " 'آزرده جان',\n",
              " 'آزرده جان',\n",
              " 'آزرده خاطر',\n",
              " 'آزرده دل',\n",
              " 'آزرده دل',\n",
              " 'آزردگی',\n",
              " 'آزردگی',\n",
              " 'آزرم',\n",
              " 'آزرم',\n",
              " 'آزرم',\n",
              " 'آزرم',\n",
              " 'آزرم',\n",
              " 'آزرمجو',\n",
              " 'آزرمجو',\n",
              " 'آزرمجو',\n",
              " 'آزرمناک',\n",
              " 'آزرمگین',\n",
              " 'آزرمگین',\n",
              " 'آزری',\n",
              " 'آزری',\n",
              " 'آزفنداک',\n",
              " 'آزما',\n",
              " 'آزما',\n",
              " 'آزمایش',\n",
              " 'آزمایش',\n",
              " 'آزمایش',\n",
              " 'آزمایش',\n",
              " 'آزمایش',\n",
              " 'آزمایشگاه',\n",
              " 'آزمایشگر',\n",
              " 'آزمند',\n",
              " 'آزمند',\n",
              " 'آزمودن',\n",
              " 'آزمودن',\n",
              " 'آزمودن',\n",
              " 'آزمودن',\n",
              " 'آزموده',\n",
              " 'آزموده',\n",
              " 'آزموده',\n",
              " 'آزموده',\n",
              " 'آزمودگی',\n",
              " 'آزمون',\n",
              " 'آزمون',\n",
              " 'آزمون',\n",
              " 'آزمون',\n",
              " 'آزمونه',\n",
              " 'آزمونه',\n",
              " 'آزندن',\n",
              " 'آزور',\n",
              " 'آزوغه',\n",
              " 'آزوق',\n",
              " 'آزوقه',\n",
              " 'آزپیشه',\n",
              " 'آزگار',\n",
              " 'آزگار',\n",
              " 'آزیدن',\n",
              " 'آزیر',\n",
              " 'آسا',\n",
              " 'آسا',\n",
              " 'آسا',\n",
              " 'آسا',\n",
              " 'آسان',\n",
              " 'آسانسور',\n",
              " 'آسان',\n",
              " 'آسان',\n",
              " 'آسان گذار',\n",
              " 'آسان گوار',\n",
              " 'آسان گیر',\n",
              " 'آسان یاب',\n",
              " 'آسایش',\n",
              " 'آسایش',\n",
              " 'آسایشگاه',\n",
              " 'آسایشگاه',\n",
              " 'آسایش جو',\n",
              " 'آسایش جو',\n",
              " 'آساینده',\n",
              " 'آساینده',\n",
              " 'آساییدن',\n",
              " 'آسا',\n",
              " 'آسا',\n",
              " 'آسبان',\n",
              " 'آستات',\n",
              " 'آستان',\n",
              " 'آستان',\n",
              " 'آستان',\n",
              " 'آستان',\n",
              " 'آستان',\n",
              " 'آستانه',\n",
              " 'آستانه',\n",
              " 'آستانه',\n",
              " 'آستانه',\n",
              " 'آستانه',\n",
              " 'آستر',\n",
              " 'آستر',\n",
              " 'آستر',\n",
              " 'آستر',\n",
              " 'آستن',\n",
              " 'آستی',\n",
              " 'آستیلن',\n",
              " 'آستیم',\n",
              " 'آستیم',\n",
              " 'آستین',\n",
              " 'آستین',\n",
              " 'آستین',\n",
              " 'آستینه',\n",
              " 'آستیگماتیسم',\n",
              " 'آسخانه',\n",
              " 'آسدست',\n",
              " 'آسغدن',\n",
              " 'آسغدن',\n",
              " 'آسغده',\n",
              " 'آسغده',\n",
              " 'آسفالت',\n",
              " 'آسفالت',\n",
              " 'آسم',\n",
              " 'آسمان',\n",
              " 'آسمان',\n",
              " 'آسمان',\n",
              " 'آسمان',\n",
              " 'آسمان',\n",
              " 'آسمانه',\n",
              " 'آسمانه',\n",
              " 'آسمان',\n",
              " 'آسمان',\n",
              " 'آسمان',\n",
              " 'آسمان',\n",
              " 'آسمان',\n",
              " 'آسمان جون',\n",
              " 'آسمان خراش',\n",
              " 'آسمان دره',\n",
              " 'آسمان رند',\n",
              " 'آسمان سا',\n",
              " 'آسمان سنگ',\n",
              " 'آسمان غرش',\n",
              " 'آسمان فرسا',\n",
              " 'آسمان نورد',\n",
              " 'آسمان پیسه',\n",
              " 'آسمان گون',\n",
              " 'آسمند',\n",
              " 'آسمند',\n",
              " 'آسموغ',\n",
              " 'آسنی',\n",
              " 'آسه',\n",
              " 'آسه',\n",
              " 'آسه',\n",
              " 'آسه',\n",
              " 'آسودن',\n",
              " 'آسودن',\n",
              " 'آسوده',\n",
              " 'آسوده',\n",
              " 'آسوده خاطر',\n",
              " 'آسوده دل',\n",
              " 'آسودگی',\n",
              " 'آسودگی',\n",
              " 'آسودگی',\n",
              " 'آسوری',\n",
              " 'آسوندار',\n",
              " 'آسپیرین',\n",
              " 'آسکاریس',\n",
              " 'آسکده',\n",
              " 'آسگون',\n",
              " 'آسی',\n",
              " 'آسیا',\n",
              " 'آسیا',\n",
              " 'آسیا',\n",
              " 'آسیاآژن',\n",
              " 'آسیاب',\n",
              " 'آسیاب',\n",
              " 'آسیاخانه',\n",
              " 'آسیازنه',\n",
              " 'آسیاسنگ',\n",
              " 'آسیاچرخ',\n",
              " 'آسیاکده',\n",
              " 'آسیب',\n",
              " 'آسیب',\n",
              " 'آسیستان',\n",
              " 'آسیمه',\n",
              " 'آسیمه سر',\n",
              " 'آسینه',\n",
              " 'آسیون',\n",
              " 'آس افزون',\n",
              " 'آس وپاس',\n",
              " 'آشام',\n",
              " 'آشام',\n",
              " 'آشام',\n",
              " 'آشام',\n",
              " 'آشامنده',\n",
              " 'آشامیدن',\n",
              " 'آشامیدن',\n",
              " 'آشامیده',\n",
              " 'آشانه',\n",
              " 'آشتی',\n",
              " 'آشتی پذیر',\n",
              " 'آشتی پذیر',\n",
              " 'آشتی کنان',\n",
              " 'آشتی کنان',\n",
              " 'آشخال',\n",
              " 'آشردن',\n",
              " 'آشرمه',\n",
              " 'آشغال',\n",
              " 'آشغال',\n",
              " 'آشغالدان',\n",
              " 'آشغالدان',\n",
              " 'آشغال جمع کن',\n",
              " 'آشغال جمع کن',\n",
              " 'آشغال کله',\n",
              " 'آشغال کله',\n",
              " 'آشفتن',\n",
              " 'آشفتن',\n",
              " 'آشفتن',\n",
              " 'آشفتن',\n",
              " 'آشفته',\n",
              " 'آشفته',\n",
              " 'آشفته',\n",
              " 'آشفته',\n",
              " 'آشفته',\n",
              " 'آشفته بخت',\n",
              " 'آشفته حال',\n",
              " 'آشفته حال',\n",
              " 'آشفته حال',\n",
              " 'آشفته خاطر',\n",
              " 'آشفته خو',\n",
              " 'آشفته دل',\n",
              " 'آشفته دماغ',\n",
              " 'آشفته دماغ',\n",
              " 'آشفته رای',\n",
              " 'آشفته روز',\n",
              " 'آشفته سامان',\n",
              " 'آشفته سامان',\n",
              " 'آشفته عقل',\n",
              " 'آشفته عقل',\n",
              " 'آشفته مو',\n",
              " 'آشفته هوش',\n",
              " 'آشفتگی',\n",
              " 'آشمال',\n",
              " 'آشمال',\n",
              " 'آشمالی',\n",
              " 'آشمیدن',\n",
              " 'آشنا',\n",
              " 'آشنا',\n",
              " 'آشنا',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = list(set(corpo))\n",
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLtriJNNo35v",
        "outputId": "b01714c2-eb7d-4b66-ffe7-c1c82bf8789f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "845417"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6WyRO7chWA6I"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roles = [x[2] for x in hazm.words_list()]\n",
        "roles = list(set(itertools.chain(*roles)))\n",
        "roles"
      ],
      "metadata": {
        "id": "febyfjwQr0hc",
        "outputId": "95a1bcc1-2285-4e83-a37a-e5f2a2d7098a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RES',\n",
              " 'PRO',\n",
              " 'INT',\n",
              " 'P',\n",
              " 'DET',\n",
              " 'AJC',\n",
              " 'PS',\n",
              " 'POSTP',\n",
              " 'ZVR',\n",
              " 'PL',\n",
              " 'NIN',\n",
              " 'CL',\n",
              " 'ADV',\n",
              " 'NUM',\n",
              " 'V',\n",
              " 'N',\n",
              " 'AJ',\n",
              " 'PR',\n",
              " 'COMP',\n",
              " 'CONJ']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "add_list = [\"ند\",\"یم\",\"ید\",\"ی\",\"تان\",\"ش\",\"ت\",\"شان\",\"مان\",\"م\",\"‍ها\",\"ات\",\"ان\",\"ین\",\"ون\"]\n",
        "corpu = []\n",
        "for word in corpus:\n",
        "  corpu.append(word)\n",
        "  corpu.append(lemmatizer.lemmatize(word))\n",
        "  for ele in add_list:\n",
        "    corpu.append(word+ele)\n",
        "\n",
        "len(corpu)"
      ],
      "metadata": {
        "id": "SlN7tsHwpAyJ",
        "outputId": "a67ea462-e2d8-4521-ebb1-cb86560a8b81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14372089"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = add_list"
      ],
      "metadata": {
        "id": "sUJqnvFEwe3v"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus=[hazm.Lemmatizer().lemmatize(word[0]) for word in hazm.words_list()]"
      ],
      "metadata": {
        "id": "JLbk2bhIaDAN"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(hazm.words_list()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swMnzRAzbtif",
        "outputId": "ccb97084-8246-4466-9087-c8edea124370"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=[word[0] for word in hazm.words_list()]"
      ],
      "metadata": {
        "id": "XtMfJGacdCy2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'corpus':corpus,\n",
        "    'pipe':nlp,\n",
        "    'stop_words':stop_words,\n",
        "    'normalizer':hazm.Normalizer(),\n",
        "    'lemmatizer':hazm.Lemmatizer(),\n",
        "    'POS_tagger':hazm.POSTagger(model='resources/postagger.model'),\n",
        "    'translator':None,\n",
        "    'word_tokenizer':hazm.WordTokenizer(),\n",
        "    'sent_tokenizer':hazm.SentenceTokenizer()\n",
        "}"
      ],
      "metadata": {
        "id": "qP0mg4wDOMIm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from hazm import *\n",
        "# tagger = POSTagger(model='Nlp HW2/resources/postagger.model')"
      ],
      "metadata": {
        "id": "LZeTOSNhui0Z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class foreign_word_detector():\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      args\n",
        "      ):\n",
        "    self.corpus = args['corpus']\n",
        "    self.stop_words = args['stop_words']\n",
        "    self.lang_pipe = args['pipe']\n",
        "    self.normalizer = args['normalizer']\n",
        "    self.lemmatizer = args['lemmatizer']\n",
        "    self.translator = args['translator']\n",
        "    self.word_tokenizer = args['word_tokenizer']\n",
        "    self.sent_tokenizer = args['sent_tokenizer']\n",
        "    self.tagger = args['POS_tagger']\n",
        "\n",
        "  def detect(self, text):\n",
        "    sentenced = self.sent_tokenizer.tokenize(text)\n",
        "    print(f\"Sentence: {sentenced}\")\n",
        "    sentenced = [sent.replace(\"\\u200c\",\" \") for sent in sentenced]\n",
        "    print(f\"Removed shift space: {sentenced}\")\n",
        "    normalized = [self.normalizer.normalize(sent) for sent in sentenced]\n",
        "    print(f\"normalized: {normalized}\")\n",
        "    lemmed = [str(self.lang_pipe(sent)) for sent in normalized]\n",
        "    print(f\"Lemmed: {lemmed}\")\n",
        "\n",
        "    words_list = [self.word_tokenizer.tokenize(sent) for sent in lemmed]\n",
        "\n",
        "    final_words_list = []\n",
        "    for sent in words_list:\n",
        "      words_second_list = []\n",
        "      tags = self.tagger.tag(sent)\n",
        "      for i in range(len(tags)):\n",
        "        if tags[i][1] != \"V\":\n",
        "          words_second_list.append(tags[i][0])\n",
        "      final_words_list.append(words_second_list)\n",
        "\n",
        "    words_list = list(itertools.chain(*final_words_list))\n",
        "    words_list = [word for word in words_list if word not in self.stop_words]\n",
        "    # print(words_list)\n",
        "    #lemmed_words_list = [lemmatizer.lemmatize(word) for word in words_list if word not in self.stop_words]\n",
        "    \n",
        "\n",
        "    output = dict()\n",
        "    out = []\n",
        "\n",
        "    for word in words_list:\n",
        "      # print(word+\" \"+str(word in self.corpus))\n",
        "      if word not in self.corpus:\n",
        "\n",
        "        out.append(word)\n",
        "        begin = text.find(word)\n",
        "        output[word] = [begin,begin+len(word)]\n",
        "    \n",
        "    return out, output\n",
        "          "
      ],
      "metadata": {
        "id": "abisr-pQyq1p"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "  detector = foreign_word_detector(args)\n",
        "\n",
        "  text = \"\"\n",
        "  print(\"enter your sample text.\\nenter an empty string if you want to end the process\")\n",
        "  while True : \n",
        "    text = input()\n",
        "    if text == \"\": \n",
        "      break\n",
        "    \n",
        "    out, output = detector.detect(text)\n",
        "    # print(out)\n",
        "    # print(output)\n"
      ],
      "metadata": {
        "id": "gFnE-4jRs3Hp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvJUGGdQw1mH",
        "outputId": "8123251c-d8fa-4e7f-abd2-2562fd531c89"
      },
      "execution_count": 68,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter your sample text.\n",
            "enter an empty string if you want to end the process\n",
            "بیا بریم به اسکول آن ها\n",
            "Sentence: ['بیا بریم به اسکول آن ها']\n",
            "Removed shift space: ['بیا بریم به اسکول آن ها']\n",
            "normalized: ['بیا بریم به اسکول آن\\u200cها']\n",
            "Lemmed: ['بیا بریم به اسکول آن\\u200cها ']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only task remained; would be to spot words which are foreign but also exist in our corpus."
      ],
      "metadata": {
        "id": "4ftp21Xf7JL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in corpus:\n",
        "  if i==\"\":\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "BBLjiYxjUXEU"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"تسک سختی بود ولی تو کانتریبیوشن خوبی داشتی .\"\n",
        "nlp(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTj03Cw1Uzxf",
        "outputId": "918b1667-97f6-4c46-bcc7-d23fade01bf8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "تسک سختی بود ولی تو کانتریبیوشن خوبی داشتی . "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp(\"من و فرزندم در گاردنمان قدم زدیم.\")"
      ],
      "metadata": {
        "id": "_WSN_wuYmRMT",
        "outputId": "28caafb6-9870-49b4-cc1e-3382c890bda6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "من و فرزند م در گاردن مان قدم زدیم . "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def change_english_words(text):\n",
        "  from googletrans import Translator\n",
        "  translator = Translator()\n",
        "  detector = foreign_word_detector(args)\n",
        "  out, output = detector.detect(text)\n",
        "  print(f\"Out: {out}\")  \n",
        "  print(f\"Output: {output}\")\n",
        "  english_text=text\n",
        "  new_text=text\n",
        "  for word in output.keys():\n",
        "    first_index=output[word][0]\n",
        "    second_index=output[word][1]\n",
        "    english_form=translator.translate(word).text\n",
        "    persian_translation=translator.translate(english_form, dest='fa').text\n",
        "    persian_translation=persian_translation.split(\"-\")[0]\n",
        "    new_text=new_text.replace(word,persian_translation)\n",
        "  return new_text,english_text\n",
        "    \n"
      ],
      "metadata": {
        "id": "8VQ2E4dfLgEM"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"تسک سختی بود ولی تو کانتریبیوشن خوبی داشتی .\"\n",
        "# text= \"سلام من و فرزندت در گاردنمان همبرگر و استیک خوریدم و آن‌ ها خیلی دلیشز بودند .\"\n",
        "translation,english_text=change_english_words(text)\n",
        "print(translation)\n",
        "# print(english_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsGwAWm4Maka",
        "outputId": "fe49de9a-2c45-48d3-dbc9-21313285b674"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: ['تسک سختی بود ولی تو کانتریبیوشن خوبی داشتی .']\n",
            "Removed shift space: ['تسک سختی بود ولی تو کانتریبیوشن خوبی داشتی .']\n",
            "normalized: ['تسک سختی بود ولی تو کانتریبیوشن خوبی داشتی.']\n",
            "Lemmed: ['تسک سختی بود ولی تو کانتریبیوشن خوبی داشتی . ']\n",
            "Out: ['تسک', 'کانتریبیوشن', '.']\n",
            "Output: {'تسک': [0, 3], 'کانتریبیوشن': [20, 31], '.': [43, 44]}\n",
            "وظیفه سختی بود ولی تو مشارکت خوبی داشتی .\n"
          ]
        }
      ]
    }
  ]
}