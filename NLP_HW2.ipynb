{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlRSiZqcpT+xA+aB94X+M9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAhmadS/NLP_HW2/blob/main/NLP_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKugniAeQcGt",
        "outputId": "37ad2951-4171-4b16-e748-7b459c194027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/NLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRCx-_IwWvxT",
        "outputId": "0e787276-0103-49ad-aa98-533475f5074c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1Ikn75GANDVFyb0klJe4-EBQJikt4_Ob_/NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extracting essential data\n",
        "\n"
      ],
      "metadata": {
        "id": "LYf9wqwEYwO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q hazm\n",
        "!pip install -q transformers"
      ],
      "metadata": {
        "id": "znLvol8uYzV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q dadmatools"
      ],
      "metadata": {
        "id": "QxyzYuMBrrF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "import hazm\n",
        "from hazm import *\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer\n"
      ],
      "metadata": {
        "id": "vJ1FDzA_Y5VW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('./extracted-phrases.json.zip','r',encoding='utf-8') as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "b6WyrXTxmsGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main code"
      ],
      "metadata": {
        "id": "ezt1J32nymfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools"
      ],
      "metadata": {
        "id": "QyhmD1vSz_F-"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dadmatools.models.normalizer import Normalizer\n",
        "\n",
        "dadma_normalizer = Normalizer(\n",
        "    full_cleaning=True,\n",
        "    unify_chars=True,\n",
        "    refine_punc_spacing=True,\n",
        "    remove_extra_space=True,\n",
        "    remove_puncs=True,\n",
        "    remove_html=False,\n",
        "    remove_stop_word=False,\n",
        "    replace_email_with=\"<EMAIL>\",\n",
        "    replace_number_with=\"<NUM>\",\n",
        "    replace_url_with=\"\",\n",
        "    replace_mobile_number_with=None,\n",
        "    replace_emoji_with=None,\n",
        "    replace_home_number_with=None,\n",
        ")\n",
        "\n",
        "dadma_normalizer.normalize(\"بیا ای عقل کل با من که بردابرد او بینی\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gRWJMy0nrWju",
        "outputId": "e91f9033-a8ed-4a92-f15f-62365aac6223"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'عقل بردابرد بینی'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [x[0] for x in hazm.words_list()]"
      ],
      "metadata": {
        "id": "_9XeR8XowdUh"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class foreign_word_detector():\n",
        "\n",
        "  def __init__(self, corpus, lemmatizer, word_tokenizer, sent_tokenizer, normalizer,translator = None):\n",
        "    self.corpus = [lemmatizer.lemmatize(x) for x in corpus]\n",
        "    self.lemmatizer = lemmatizer\n",
        "    self.normalizer = normalizer\n",
        "    self.translator = translator\n",
        "    self.word_tokenizer = word_tokenizer\n",
        "    self.sent_tokenizer = sent_tokenizer\n",
        "\n",
        "  def detect(self, text):\n",
        "    sentenced = self.sent_tokenizer.tokenize(text)\n",
        "    normalized = [self.normalizer.normalize(sent) for sent in sentenced]\n",
        "    words_list = [self.word_tokenizer.tokenize(sent) for sent in normalized]\n",
        "    words_list = [self.lemmatizer.lemmatize(x) for x in itertools.chain(*words_list)]\n",
        "\n",
        "    output = dict()\n",
        "    out = []\n",
        "    for word in words_list:\n",
        "      if word not in self.corpus:\n",
        "        out.append(word)\n",
        "        begin = text.find(word)\n",
        "        output[word] = [begin,begin+len(word)]\n",
        "    \n",
        "    return out, output\n",
        "    "
      ],
      "metadata": {
        "id": "abisr-pQyq1p"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run():\n",
        "  detector = foreign_word_detector(corpus, hazm.Lemmatizer(), hazm.WordTokenizer(), hazm.SentenceTokenizer(), dadma_normalizer)\n",
        "\n",
        "  text = \"\"\n",
        "  print(\"enter your sample text.\\nenter an empty string if you want to end the process\")\n",
        "  while True : \n",
        "    text = input()\n",
        "    if text == \"\": \n",
        "      break\n",
        "    \n",
        "    out, output = detector.detect(text)\n",
        "    print(out)\n",
        "    print(output)\n"
      ],
      "metadata": {
        "id": "gFnE-4jRs3Hp"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvJUGGdQw1mH",
        "outputId": "5c531467-4015-4fcb-d2a2-f9060a189500"
      },
      "execution_count": 106,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter your sample text.\n",
            "enter an empty string if you want to end the process\n",
            "سیستم کامپیوترم خراب شده.\n",
            "['سیستم', 'کامپیوترم']\n",
            "{'سیستم': [0, 5], 'کامپیوترم': [6, 15]}\n",
            "امروز یک کار خیلی هاردی داشتیم ولی تو کانتریبیوشن خوبی داشتی تنکس.\n",
            "['کانتریبیوشن', 'تنکس']\n",
            "{'کانتریبیوشن': [38, 49], 'تنکس': [61, 65]}\n",
            "تسک امروز هارد بود\n",
            "['تسک', 'هارد']\n",
            "{'تسک': [0, 3], 'هارد': [10, 14]}\n",
            "تسک امروز هاردی بود\n",
            "['تسک']\n",
            "{'تسک': [0, 3]}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem is: using stemmer, we get lots of errors; while using lemmatizer, pronouns are not ommited from the word.\n",
        "\n",
        "\n",
        "one way to deal with it would be to apply stemming right after dealing with the word origin.\n",
        "\n",
        "another way would be to find a way to work with the tokenizer introduced in the next cell, since it gives us specific features."
      ],
      "metadata": {
        "id": "4ftp21Xf7JL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bolbol_tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRIPmkiJZiGH",
        "outputId": "b95633e4-8980-48fe-cdbc-666ce47370df"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bolbol_tokenizer.tokenize(\"سیستم کامپیوترم خراب شده.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34SAQexo8gIL",
        "outputId": "8d557c8d-1ace-4cb8-db9a-b245a9502a92"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁سیستم', '▁کامپیوتر', 'م', '▁خراب', '▁شده', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    }
  ]
}