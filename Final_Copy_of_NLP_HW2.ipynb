{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAhmadS/NLP_HW2/blob/main/Final_Copy_of_NLP_HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKugniAeQcGt",
        "outputId": "c080c412-953c-42e9-85a0-671519d697c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/NLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRCx-_IwWvxT",
        "outputId": "2e7f604b-fdfa-4a43-ba12-421cc04a9131"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/1Ikn75GANDVFyb0klJe4-EBQJikt4_Ob_/NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Extracting essential data\n",
        "\n"
      ],
      "metadata": {
        "id": "LYf9wqwEYwO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Essential packages"
      ],
      "metadata": {
        "id": "njh0oGO4Nr7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "n2iCwQ7T3uBu"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#installing the necessary libraries\n",
        "!pip install -q hazm\n",
        "!pip install -q dadmatools"
      ],
      "metadata": {
        "id": "znLvol8uYzV-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the necessary libraries\n",
        "from __future__ import unicode_literals\n",
        "import hazm\n",
        "\n",
        "import dadmatools.pipeline.language as language\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "vJ1FDzA_Y5VW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating dataset"
      ],
      "metadata": {
        "id": "QU-8QbkZN4Pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Nlp HW2/corpus.txt\",\"rb\") as f:\n",
        "  corpo = pickle.load(f)\n",
        "\n",
        "corpo"
      ],
      "metadata": {
        "id": "wtvAazXLhkjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the corpus used for this task comes from hazm word list\n",
        "corpus = list(set(corpo))\n",
        "len(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLtriJNNo35v",
        "outputId": "63ad65a8-e724-4d28-f6d5-55c78abb644b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35316"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#roles contains all the possible roles in our dictionary\n",
        "roles = [x[2] for x in hazm.words_list()]\n",
        "roles = list(set(itertools.chain(*roles)))\n",
        "roles"
      ],
      "metadata": {
        "id": "febyfjwQr0hc",
        "outputId": "8fd51df9-882b-4f9a-e7c2-0425d9a1c92e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['N',\n",
              " 'COMP',\n",
              " 'POSTP',\n",
              " 'V',\n",
              " 'P',\n",
              " 'PL',\n",
              " 'NIN',\n",
              " 'NUM',\n",
              " 'INT',\n",
              " 'AJC',\n",
              " 'PR',\n",
              " 'ADV',\n",
              " 'DET',\n",
              " 'CL',\n",
              " 'CONJ',\n",
              " 'PRO',\n",
              " 'RES',\n",
              " 'PS',\n",
              " 'ZVR',\n",
              " 'AJ']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = hazm.Lemmatizer()\n",
        "stemmer = hazm.Stemmer()"
      ],
      "metadata": {
        "id": "LZ0cqll7Zwdu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''this way of defining stop_words may not be usual,\n",
        "but the main purpose has been to eliminate disturbing words;\n",
        "such as: punctiation marks, pronouns and Persian\\Arabic grammatical signs'''\n",
        "\n",
        "add_list = [\"ند\",\"یم\",\"ید\",\"ی\",\"تان\",\"ش\",\"ت\",\"شان\",\"مان\",\"م\",\"‍ها\",\"ات\",\"ان\",\"ین\",\"ون\"]\n",
        "stop_words = add_list\n",
        "stop_words.extend([\".\",\"؛\",\",\",\":\",\"?\",\"!\",\"؟\",\"ْ\",\"ٌ\",\"ٍ\",\"ً\",\"ُ\",\"ِ\",\"َ\",\"ّ\",\"}\",\"ؤ\",\"{\",\"»\",\"«\",\"ٰ\",\"‌‌‌‌‌‌‌‌ٔ\",\"ء\",\"\\\"\"])"
      ],
      "metadata": {
        "id": "sUJqnvFEwe3v"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpu = []\n",
        "for word in corpus:\n",
        "  corpu.append(word)\n",
        "  corpu.append(lemmatizer.lemmatize(word))\n",
        "  for ele in add_list:\n",
        "    corpu.append(word+ele)\n",
        "\n",
        "len(corpu)"
      ],
      "metadata": {
        "id": "SlN7tsHwpAyJ",
        "outputId": "190a5bcd-6001-4c91-b037-2ef4696e37ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600372"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Main code"
      ],
      "metadata": {
        "id": "ezt1J32nymfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Config"
      ],
      "metadata": {
        "id": "uoztZzu6NAYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nlp is the dadmatools' pipeline used for lemmatization purpose only\n",
        "pips = 'lem' \n",
        "nlp = language.Pipeline(pips)\n",
        "\n",
        "print(nlp.analyze_pipes(pretty=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ShvxXYdXJ0Y",
        "outputId": "accdb6c3-f817-49d5-973c-978f926826dd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading file /root/.pernlp/fa_tokenizer.pt: : 639kB [00:01, 438kB/s]                           \n",
            "Downloading file /root/.pernlp/fa_mwt.pt: : 721kB [00:01, 526kB/s]                           \n",
            "Downloading file /root/.pernlp/fa_lemmatizer.pt: : 4.69MB [00:01, 3.33MB/s]                        \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================= Pipeline Overview =============================\u001b[0m\n",
            "\n",
            "#   Component   Assigns       Requires   Scores   Retokenizes\n",
            "-   ---------   -----------   --------   ------   -----------\n",
            "0   tokenizer                                     True       \n",
            "                                                             \n",
            "1   lemmatize   token.lemma                       False      \n",
            "\n",
            "\u001b[38;5;2m✔ No problems found.\u001b[0m\n",
            "{'summary': {'tokenizer': {'assigns': [], 'requires': [], 'scores': [], 'retokenizes': True}, 'lemmatize': {'assigns': ['token.lemma'], 'requires': [], 'scores': [], 'retokenizes': False}}, 'problems': {'tokenizer': [], 'lemmatize': []}, 'attrs': {'token.lemma': {'assigns': ['lemmatize'], 'requires': []}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we use an args dictionary to initialize our model\n",
        "args = {\n",
        "    'corpus':corpus,\n",
        "    'pipe':nlp,\n",
        "    'stop_words':stop_words,\n",
        "    'normalizer':hazm.Normalizer(),\n",
        "    'lemmatizer':hazm.Lemmatizer(),\n",
        "    'POS_tagger':hazm.POSTagger(model='Nlp HW2/resources/postagger.model'),\n",
        "    'translator':None,\n",
        "    'word_tokenizer':hazm.WordTokenizer(),\n",
        "    'sent_tokenizer':hazm.SentenceTokenizer()\n",
        "}"
      ],
      "metadata": {
        "id": "qP0mg4wDOMIm"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model"
      ],
      "metadata": {
        "id": "6kvuGlYaNPYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#as the name suggests, the class is used to spot and show the positin of foreign words within the text\n",
        "\n",
        "class foreign_word_detector():\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      args\n",
        "      ):\n",
        "    \n",
        "    self.corpus = args['corpus']\n",
        "    self.stop_words = args['stop_words']\n",
        "    self.lang_pipe = args['pipe']\n",
        "    self.normalizer = args['normalizer']\n",
        "    self.lemmatizer = args['lemmatizer']\n",
        "    self.translator = args['translator']\n",
        "    self.word_tokenizer = args['word_tokenizer']\n",
        "    self.sent_tokenizer = args['sent_tokenizer']\n",
        "    self.tagger = args['POS_tagger']\n",
        "\n",
        "#the following function is the main funtion of the model used for detecting the foreign words and their position\n",
        "  def detect(self, text):\n",
        "    sentenced = self.sent_tokenizer.tokenize(text)\n",
        "\n",
        "    normalized = [self.normalizer.normalize(sent) for sent in sentenced]\n",
        "    normalized = [sent.replace(\"\\u200c\",\" \") for sent in normalized]\n",
        "\n",
        "    lemmed = [str(self.lang_pipe(sent)) for sent in normalized]\n",
        "\n",
        "    words_list = [self.word_tokenizer.tokenize(sent) for sent in lemmed]\n",
        "\n",
        "    final_words_list = []\n",
        "    for sent in words_list:\n",
        "      words_second_list = []\n",
        "      tags = self.tagger.tag(sent)\n",
        "      for i in range(len(tags)):\n",
        "        if tags[i][1] != \"V\":\n",
        "          words_second_list.append(tags[i][0])\n",
        "      final_words_list.append(words_second_list)\n",
        "\n",
        "    words_list = list(itertools.chain(*final_words_list))\n",
        "    words_list = [word for word in words_list if word not in self.stop_words]\n",
        "    lemmed_words_list = [lemmatizer.lemmatize(word) for word in words_list]\n",
        "    \n",
        "\n",
        "    output = dict()\n",
        "    for word in lemmed_words_list:\n",
        "      if word not in self.corpus:\n",
        "        output[word] = []\n",
        "        add_ind = 0\n",
        "\n",
        "        string = text\n",
        "        while word in string:\n",
        "\n",
        "          begin = string.find(word)\n",
        "          output[word].append((begin+add_ind,add_ind+begin+len(word)))\n",
        "\n",
        "          string = string[begin+len(word):]\n",
        "          add_ind+= begin+len(word)\n",
        "    \n",
        "    return output\n",
        "          "
      ],
      "metadata": {
        "id": "abisr-pQyq1p"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Test"
      ],
      "metadata": {
        "id": "Oyh-F0TrNX2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####results"
      ],
      "metadata": {
        "id": "REhKcO_FRmuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.load_csv(\"Nlp HW2/test.csv\")"
      ],
      "metadata": {
        "id": "l5ixR8F9R7fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####user run"
      ],
      "metadata": {
        "id": "tljaM-3WRscu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the run function, is an interactive way to map user_input to the detectors answer\n",
        "def run():\n",
        "  detector = foreign_word_detector(args)\n",
        "\n",
        "  print(\"enter your sample text.\\nenter an empty string if you want to end the process\")\n",
        "  while True : \n",
        "    text = input()\n",
        "    if text == \"\": \n",
        "      break\n",
        "    \n",
        "    output = detector.detect(text)\n",
        "    print(output)\n"
      ],
      "metadata": {
        "id": "gFnE-4jRs3Hp"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvJUGGdQw1mH",
        "outputId": "75a657a8-e99d-479a-9a6a-48c772b7cbf2"
      },
      "execution_count": 67,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter your sample text.\n",
            "enter an empty string if you want to end the process\n",
            "سیستم کامپیوتر من خراب است؛ سیستم کامپیوتر شما چطور؟\n",
            "{'سیستم': [(0, 5), (28, 33)], 'کامپیوتر': [(6, 14), (34, 42)]}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##New idea"
      ],
      "metadata": {
        "id": "hr0gxCMLNfjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We import an English dictionary. Add the english words in \"words\" and the corresponding phonetics to \"phonics_list\".\n",
        "words = []\n",
        "phonics_list = []\n",
        "dict_file = open('Nlp HW2/cmudict.dict', 'r')\n",
        "with dict_file as f:\n",
        "    phonics = [line.rstrip('\\n') for line in f]\n",
        "    for p in phonics:\n",
        "        x = p.split(' ')\n",
        "        words.append(x[0])\n",
        "        phonics_list.append(' '.join(x[1:]))"
      ],
      "metadata": {
        "id": "_kzbejEP6GP7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phons_unique = list(set(itertools.chain(*[ph.split(\" \") for ph in phonics_list])))\n",
        "phons_as_list = [ph.split(\" \") for ph in phonics_list]"
      ],
      "metadata": {
        "id": "zulC4Adu6upO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"Nlp HW2/phons_exchange.txt\",\"rb\") as file:\n",
        "  phons_dict = pickle.load(file)"
      ],
      "metadata": {
        "id": "vtwOxqizjC-X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phons_dict[\"NG\"] = (\"ن\")\n",
        "phons_dict[\"TH\"] = (\"ت\")\n",
        "phons_dict[\"AO2\"] = phons_dict[\"AO1\"]\n",
        "phons_dict[\"IH2\"] = (\"ای\",\"ی\")\n",
        "phons_dict[\"IH0\"] = phons_dict[\"IH2\"]\n",
        "phons_dict[\"IH1\"] = phons_dict[\"IH0\"]"
      ],
      "metadata": {
        "id": "6_UTCNuVjWlr"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting our words to their persian correspondent with the use of phonetics in the \"persianized\"\n",
        "persianized_list = []\n",
        "persianized = []\n",
        "for x in phons_as_list:\n",
        "  word_list = []\n",
        "  count=0\n",
        "  for ph in x:\n",
        "    count+=1\n",
        "    if len(phons_dict[ph])>1:\n",
        "      if count==1:\n",
        "        word_list.append(phons_dict[ph][0])\n",
        "      else:\n",
        "        word_list.append(phons_dict[ph][1])\n",
        "    else:\n",
        "      word_list.append(phons_dict[ph][0])\n",
        "\n",
        "  persianized_list.append(word_list)\n",
        "  persianized.append(''.join(word_list))"
      ],
      "metadata": {
        "id": "TZQlhU9GjSJi"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking some words from \"words\"\n",
        "words[41000:41010]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_pSldPjknPo",
        "outputId": "3179fa58-89b9-44e5-abfd-1a614d3b6b9f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fancier',\n",
              " 'fanciers',\n",
              " 'fancies',\n",
              " 'fanciest',\n",
              " 'fanciful',\n",
              " 'fancy',\n",
              " 'fandango',\n",
              " 'fandel',\n",
              " 'fandrich',\n",
              " 'fane']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the corresponding persionized words from \"persionized\"\n",
        "persianized[41000:41010]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mKUNAPmkdt9",
        "outputId": "8c23cbc4-9c99-4c61-864e-6285923e1b1d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['فنسير',\n",
              " 'فنسيرز',\n",
              " 'فنسيز',\n",
              " 'فنسيست',\n",
              " 'فنسیفل',\n",
              " 'فنسي',\n",
              " 'فندنگو',\n",
              " 'فندل',\n",
              " 'فندریک',\n",
              " 'فین']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if our persionized method works fine on some words.\n",
        "listt = ['contribution','thanks','computer','system','task','time','garden','burger','chop','fandrich','infinity']\n",
        "\n",
        "for i in range(len(words)):\n",
        "  if words[i] in listt:\n",
        "    print(words[i])\n",
        "    print(phons_as_list[i])\n",
        "    print(i)\n",
        "    print(persianized[i])\n",
        "    print(\"-----------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm2B0nXskxX8",
        "outputId": "c6c0cfff-990b-4df3-c110-74d4379e721b"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "burger\n",
            "['B', 'ER1', 'G', 'ER0']\n",
            "16631\n",
            "برگر\n",
            "-----------------\n",
            "chop\n",
            "['CH', 'AA1', 'P']\n",
            "21283\n",
            "چاپ\n",
            "-----------------\n",
            "computer\n",
            "['K', 'AH0', 'M', 'P', 'Y', 'UW1', 'T', 'ER0']\n",
            "23974\n",
            "کمپیوتر\n",
            "-----------------\n",
            "contribution\n",
            "['K', 'AA2', 'N', 'T', 'R', 'AH0', 'B', 'Y', 'UW1', 'SH', 'AH0', 'N']\n",
            "25006\n",
            "کانتربیوشن\n",
            "-----------------\n",
            "fandrich\n",
            "['F', 'AE1', 'N', 'D', 'R', 'IH0', 'K']\n",
            "41008\n",
            "فندریک\n",
            "-----------------\n",
            "garden\n",
            "['G', 'AA1', 'R', 'D', 'AH0', 'N']\n",
            "46458\n",
            "گاردن\n",
            "-----------------\n",
            "infinity\n",
            "['IH2', 'N', 'F', 'IH1', 'N', 'IH0', 'T', 'IY0']\n",
            "59556\n",
            "اینفینیتي\n",
            "-----------------\n",
            "system\n",
            "['S', 'IH1', 'S', 'T', 'AH0', 'M']\n",
            "118902\n",
            "سیستم\n",
            "-----------------\n",
            "task\n",
            "['T', 'AE1', 'S', 'K']\n",
            "119638\n",
            "تسک\n",
            "-----------------\n",
            "thanks\n",
            "['TH', 'AE1', 'NG', 'K', 'S']\n",
            "120780\n",
            "تنکس\n",
            "-----------------\n",
            "time\n",
            "['T', 'AY1', 'M']\n",
            "121710\n",
            "تایم\n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#phons_dict contains the Persian equivalent of English phonetics\n",
        "phons_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gz8wC5fCZrn",
        "outputId": "c78fec42-a94a-4f97-d2bf-19d84a8ca833"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'EH0': ('ا', ''),\n",
              " 'ER2': ('ار', 'ر'),\n",
              " 'old': ('',),\n",
              " 'EY0': ('ای', 'ی'),\n",
              " 'IY2': ('ی',),\n",
              " 'name': ('',),\n",
              " 'S': ('س',),\n",
              " 'CH': ('چ',),\n",
              " 'K': ('ک',),\n",
              " 'UW2': ('یو',),\n",
              " 'AA0': ('آ', 'ا'),\n",
              " 'AY2': ('آی', 'ای'),\n",
              " 'OY0': ('اوی', 'وی'),\n",
              " 'AA1': ('آ', 'ا'),\n",
              " 'L': ('ل',),\n",
              " 'UH0': ('و',),\n",
              " 'B': ('ب',),\n",
              " 'F': ('ف',),\n",
              " 'OY1': ('اوی', 'وی'),\n",
              " 'abbrev': ('',),\n",
              " 'UH2': ('و',),\n",
              " 'D': ('د',),\n",
              " 'JH': ('ج',),\n",
              " 'W': ('و',),\n",
              " 'G': ('گ',),\n",
              " 'IH0': ('ی',),\n",
              " 'UH1': ('و',),\n",
              " 'french': ('',),\n",
              " 'Z': ('ز',),\n",
              " 'AA2': ('آ', 'ا'),\n",
              " 'OY2': ('اوی', 'وی'),\n",
              " 'SH': ('ش',),\n",
              " 'AH0': ('ا', ''),\n",
              " 'foreign': ('',),\n",
              " 'UW0': ('یو',),\n",
              " 'AH1': ('ا', ''),\n",
              " 'IH1': ('ی',),\n",
              " 'EY2': ('ای', 'ی'),\n",
              " 'AH2': ('ا', ''),\n",
              " 'OW2': ('او', 'و'),\n",
              " 'Y': ('ی',),\n",
              " 'AE2': ('',),\n",
              " 'DH': ('ض',),\n",
              " 'P': ('پ',),\n",
              " 'ER1': ('ار', 'ر'),\n",
              " 'AW2': ('او',),\n",
              " '#': ('',),\n",
              " 'EH1': ('ا', ''),\n",
              " 'AE1': ('',),\n",
              " 'EH2': ('ا', ''),\n",
              " 'N': ('ن',),\n",
              " 'ZH': ('ز',),\n",
              " 'OW1': ('او', 'و'),\n",
              " 'UW1': ('او', 'و'),\n",
              " 'AY0': ('آی', 'ای'),\n",
              " 'AY1': ('آی', 'ای'),\n",
              " 'AE0': ('',),\n",
              " 'NG': 'ن',\n",
              " 'AO1': ('او', 'و'),\n",
              " 'OW0': ('او', 'و'),\n",
              " 'HH': ('ه',),\n",
              " 'AW0': ('او',),\n",
              " 'TH': 'ت',\n",
              " 'IY0': ('ي',),\n",
              " 'AO0': ('او', 'و'),\n",
              " 'AW1': ('او',),\n",
              " 'R': ('ر',),\n",
              " 'AO2': ('او', 'و'),\n",
              " 'M': ('م',),\n",
              " 'T': ('ت',),\n",
              " 'IH2': ('ی',),\n",
              " 'V': ('و',),\n",
              " 'ER0': ('ار', 'ر'),\n",
              " 'IY1': ('ی',),\n",
              " 'EY1': ('ای', 'ی')}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BWa2xvzrNhFs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}